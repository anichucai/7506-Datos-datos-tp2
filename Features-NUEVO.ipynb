{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk, os, re, string, collections\n",
    "import matplotlib\n",
    "from  imageio import imread\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "import spacy\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import gc\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import pickle\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('original_data/train.csv')\n",
    "test  = pd.read_csv('original_data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) LIMPIEZA DE TEXTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) a) Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253\n"
     ]
    }
   ],
   "source": [
    "def getStopwordsList():\n",
    "    fileNamesList = [\"texts/99webtools.txt\", \"texts/atire_ncbi.txt\", \"texts/atire_puurula.txt\", \"texts/azure.txt\", \n",
    "                 \"texts/bbalet.txt\", \n",
    "                 \"texts/bow_short.txt\", \"texts/choi_2000naacl.txt\", \"texts/cook1988_function_words.txt\", \n",
    "                 \"texts/corenlp_acronym.txt\", \n",
    "                 \"texts/corenlp_hardcoded.txt\", \"texts/corenlp_stopwords.txt\", \"texts/datasciencedojo.txt\", \n",
    "                 \"texts/deeplearning4j.txt\", \n",
    "                 \"texts/dkpro.txt\", \"texts/mongodb.txt\", \"texts/galago_inquery.txt\", \"texts/gate_keyphrase.txt\", \n",
    "                 \"texts/gensim.txt\", \n",
    "                 \"texts/glasgow_stop_words.txt\", \"texts/indri.txt\", \"texts/kevinbouge.txt\", \"texts/lexisnexis.txt\",\n",
    "                 \"texts/lingpipe.txt\", \n",
    "                 \"texts/mallet.txt\", \"texts/mysql_innodb.txt\", \"texts/mysql_myisam.txt\", \"texts/galago_rmstop.txt\", \n",
    "                 \"texts/atire_ncbi.txt\", \n",
    "                 \"texts/galago_rmstop.txt\", \"texts/nltk.txt\", \"texts/okapiframework.txt\", \"texts/okapi_cacm_expanded.txt\", \n",
    "                 \"texts/onix.txt\", \n",
    "                 \"texts/ovid.txt\", \"texts/postgresql.txt\", \"texts/pubmed.txt\", \"texts/quanteda.txt\", \"texts/r_tm.txt\", \n",
    "                 \"texts/ranksnl_large.txt\", \n",
    "                 \"texts/reuters_wos.txt\", \"texts/rouge_155.txt\", \"texts/scikitlearn.txt\", \"texts/smart.txt\", \n",
    "                 \"texts/snowball_expanded.txt\", \n",
    "                 \"texts/spacy.txt\", \"texts/spark_mllib.txt\", \"texts/sphinx_mirasvit.txt\", \"texts/t101_minimal.txt\", \n",
    "                 \"texts/taporware.txt\", \n",
    "                 \"texts/terrier.txt\", \"texts/tonybsk_1.txt\", \"texts/tonybsk_6.txt\", \"texts/voyant_taporware.txt\", \n",
    "                 \"texts/weka.txt\", \n",
    "                 \"texts/xapian.txt\", \"texts/xpo6.txt\", \"texts/zettair.txt\"]\n",
    "    stopwordsList = []\n",
    "    for fileName in fileNamesList:\n",
    "        file = open(fileName, \"r\", encoding=\"utf8\")\n",
    "        for line in file:\n",
    "            stripped_line = line. strip()\n",
    "            line_list = stripped_line\n",
    "            if line_list not in stopwordsList:\n",
    "                stopwordsList.append(line_list)\n",
    "        file.close()\n",
    "    return stopwordsList\n",
    "\n",
    "stopwordsList = getStopwordsList()\n",
    "\n",
    "print(len(stopwordsList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_strict(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\d+', '')\n",
    "    invalid_chars = ['#','|','@','!','?','-','_','[',']','%','&',':','.',',',\"''\",'/','https','(','//t',')','http',\n",
    "                 ';','\\'']\n",
    "    for char in invalid_chars:\n",
    "        if char in text:\n",
    "            text = text.replace(char,' ')\n",
    "    #removes url and tags\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \" \",  text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES AUXILIARES\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def prettify(word):\n",
    "    word = re.sub(r'[^A-Za-z]', \" \",  word)\n",
    "    word = re.sub(r'\\b\\w\\b',' ', word)\n",
    "    word = re.sub(r'\\s+', ' ', word)\n",
    "    return word\n",
    "\n",
    "def cleanStopwords(tweet):\n",
    "    tweet = tweet.split(' ')\n",
    "    cleanTweet=\"\"\n",
    "    for word in tweet:\n",
    "        if word not in stopwordsList:\n",
    "            cleanTweet = cleanTweet + \" \" + word\n",
    "    return cleanTweet\n",
    "\n",
    "def clean_text_non_strict(text): \n",
    "    tw = \" \"\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        word = ''.join(filter(lambda x: x in set(string.printable), word))\n",
    "        word = word.replace(\"\\n\",\" \")\n",
    "        word = word.replace('û',\"\")\n",
    "        word = word.replace('Û',\"\")\n",
    "        #word = word.replace('_','')\n",
    "        #word = word.replace(\"\\\"\",'')\n",
    "        #word = word.strip('.')\n",
    "        #word = word.strip(',')\n",
    "        #word = word.strip(':')\n",
    "        tw += word + \" \"\n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def lemmatize_text_v2(text):\n",
    "    if text == '' or pd.isnull(text):\n",
    "        return text\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIONES AUXILIARES: Limpieza de tweet\n",
    "\n",
    "# Función que saca los links\n",
    "\n",
    "def quitar_link_twitter(tweet):\n",
    "    res = []\n",
    "    for w in tweet.split(\" \"):\n",
    "        if ((\"http\" not in w) and (\"https\" not in w)):\n",
    "            res.append(w)\n",
    "    return (\" \").join(res)\n",
    "\n",
    "# Función que saca las menciones\n",
    "\n",
    "def quitar_mencion_twitter(tweet):\n",
    "    res = []\n",
    "    for w in tweet.split(\" \"):\n",
    "        if(\"@\" not in w):\n",
    "            res.append(w)\n",
    "    return (\" \").join(res)\n",
    "\n",
    "def quitar_simbolo(tweet, simbolo):\n",
    "    text = \"\"\n",
    "    for c in tweet:\n",
    "        if(c!=simbolo):\n",
    "            text+=c\n",
    "    return text\n",
    "\n",
    "def agregar_espacio_ente_comas(tweet):\n",
    "    \n",
    "    tweer_sin_coma = (\" \").join(tweet.split(\",\"))\n",
    "\n",
    "    return (\" \").join(tweer_sin_coma.split(\".\"))\n",
    "\n",
    "def capitalize_each_word(string):\n",
    "    \n",
    "    text = \"\"\n",
    "    for w in string.split(\" \"):\n",
    "        if(len(w)>3):\n",
    "            text += w.capitalize()+\" \"\n",
    "    return text\n",
    "\n",
    "def limpiar_tweet_location(tweet):\n",
    "    res = tweet\n",
    "    func = [quitar_mencion_twitter, quitar_link_twitter, agregar_espacio_ente_comas, lambda x: quitar_simbolo(x, \"#\"), capitalize_each_word]\n",
    "    for f in func:\n",
    "        res = f(res)\n",
    "    return res\n",
    "\n",
    "def to_string(a_list, delimeter):\n",
    "    res = \"\"\n",
    "    \n",
    "    for e in sorted(filter(None, a_list)):\n",
    "        res += str(e)+delimeter\n",
    "    \n",
    "    if(len(res)==0): return None\n",
    "    return res[: -len(delimeter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addKeywordToText(x):\n",
    "    y=pd.isnull(x.keyword)\n",
    "    if (y == True):\n",
    "        return x['text']\n",
    "    else:\n",
    "        return (x['keyword'] + ' ' + x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLocationToText(x):\n",
    "    if (x.location == ' '):\n",
    "        return x['text_w_location']\n",
    "    else:\n",
    "        return (x['location'] + ' ' + x['text_w_location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) b) Text + Keyword [baja un toque el score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train[\"keyword\"] = train[\"keyword\"].str.replace('%20',' ')\n",
    "train['text_w_keyword'] = train.apply(lambda x: addKeywordToText(x), axis=1)\n",
    "\n",
    "test[\"keyword\"] = test[\"keyword\"].str.replace('%20',' ')\n",
    "test['text_w_keyword'] = test.apply(lambda x: addKeywordToText(x), axis=1)\n",
    "\n",
    "train.text_w_keyword = train[\"text_w_keyword\"].apply(lambda x: clean_text_strict(x))\n",
    "test.text_w_keyword  = test[\"text_w_keyword\"].apply(lambda x: clean_text_strict(x))\n",
    "\n",
    "train.text_w_keyword = train.text_w_keyword.apply(prettify)\n",
    "test.text_w_keyword  = test.text_w_keyword.apply(prettify)\n",
    "\n",
    "train.text_w_keyword = train.text_w_keyword.apply(lemmatize_text_v2)\n",
    "test.text_w_keyword  = test.text_w_keyword.apply(lemmatize_text_v2)\n",
    "\n",
    "train.text_w_keyword = train.text_w_keyword.apply(cleanStopwords)\n",
    "test.text_w_keyword  = test.text_w_keyword.apply(cleanStopwords)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) c) Text + Location [baja score :(]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train.location = train.location.fillna('')\n",
    "train.location = train.location.apply(lambda x:limpiar_tweet_location(x))\n",
    "train['text_w_location'] = train.apply(lambda x: addLocationToText(x), axis=1)\n",
    "\n",
    "test.location = test.location.fillna('')\n",
    "test.location = test.location.apply(lambda x:limpiar_tweet_location(x))\n",
    "test['text_w_location'] = test.apply(lambda x: addLocationToText(x), axis=1)\n",
    "\n",
    "train.text_w_location = train[\"text_w_location\"].apply(lambda x: clean_text_strict(x))\n",
    "test.text_w_location  = test[\"text_w_location\"].apply(lambda x: clean_text_strict(x))\n",
    "\n",
    "train.text_w_location = train.text_w_location.apply(prettify)\n",
    "test.text_w_location  = test.text_w_location.apply(prettify)\n",
    "\n",
    "train.text_w_location = train.text_w_location.apply(lemmatize_text_v2)\n",
    "test.text_w_location  = test.text_w_location.apply(lemmatize_text_v2)\n",
    "\n",
    "train.text_w_location = train.text_w_location.apply(cleanStopwords)\n",
    "test.text_w_location  = test.text_w_location.apply(cleanStopwords)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) d)  Sólo Texto [mejor hasta ahora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.text = train[\"text\"].apply(lambda x: quitar_link_twitter(x)) #Por algun motivo esto baja un poquito el score\n",
    "#test.text  = test[\"text\"].apply(lambda x: quitar_link_twitter(x))\n",
    "\n",
    "#train.text = train[\"text\"].apply(lambda x: quitar_mencion_twitter(x)) #Por algun motivo esto baja un poquito el score\n",
    "#test.text  = test[\"text\"].apply(lambda x: quitar_mencion_twitter(x))\n",
    "\n",
    "train.text = train[\"text\"].apply(lambda x: clean_text_non_strict(x))\n",
    "test.text  = test[\"text\"].apply(lambda x: clean_text_non_strict(x))\n",
    "\n",
    "train.text = train[\"text\"].apply(lambda x: clean_text_strict(x))\n",
    "test.text  = test[\"text\"].apply(lambda x: clean_text_strict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text = train.text.apply(prettify)\n",
    "test.text  = test.text.apply(prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a veces baja score\n",
    "#train.text = train.text.apply(lemmatize_text_v2)\n",
    "#test.text  = test.text.apply(lemmatize_text_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -PRON- deed reason earthquake allah forgive -...\n",
       "1                           forest fire ronge sask canada\n",
       "2        resident shelter notify officer evacuation sh...\n",
       "3           people receive wildfire evacuation california\n",
       "4                   photo ruby alaska smoke wildfire pour\n",
       "                              ...                        \n",
       "7608     giant crane hold bridge collapse nearby stfmb...\n",
       "7609       aria ahrary thetawniest control wild fire c...\n",
       "7610                        utc volcano hawaii zdtoyd ebj\n",
       "7611     police bike collide portugal bike rider suffe...\n",
       "7612       raze northern california wildfire abc ymy rskq\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text = train.text.apply(cleanStopwords)\n",
    "test.text  = test.text.apply(cleanStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this earthquake ma...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents asked to shelter in place are be...\n",
       "3        people receive wildfires evacuation orders in...\n",
       "4       just got sent this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding bridge collapse into ...\n",
       "7609     aria ahrary thetawniest the out of control wi...\n",
       "7610               utc km of volcano hawaii co zdtoyd ebj\n",
       "7611    police investigating after an bike collided wi...\n",
       "7612    the latest more homes razed by northern califo...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) FEATURES NUMERICOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) a) Preparation of the TRAIN and TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET - ONLY NUMERICAL\n",
    "test_keywords_numerical = pd.read_csv('csv_lio/test/keywords_numerical_features_test - Copy.csv')\n",
    "test_keywords_numerical = test_keywords_numerical.drop(columns = 'Unnamed: 0')\n",
    "test_text_numerical = pd.read_csv('csv_lio/test/text_general_numerical_features_test - Copy.csv')\n",
    "test_text_numerical = test_text_numerical.drop(columns = ['Unnamed: 0','#silabas'])\n",
    "test_hashtag_numerical = pd.read_csv('csv_lio/test/features_hashtags_numerical.csv')\n",
    "test_hashtag_numerical  =test_hashtag_numerical.drop(columns = 'Unnamed: 0')\n",
    "test_links_numerical = pd.read_csv('csv_lio/test/features_links_numerical.csv')\n",
    "test_links_numerical = test_links_numerical.drop(columns = ['id.1','target'])\n",
    "test_new_use = test.drop(columns = ['keyword','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = pd.merge(test_new_use,test_text_numerical, on = 'id', how = 'left')\n",
    "features_test = pd.merge(features_test,test_keywords_numerical, on = 'id', how = 'left')\n",
    "features_test = pd.merge(features_test,test_hashtag_numerical, on = 'id', how = 'left')\n",
    "features_test = pd.merge(features_test,test_links_numerical, on = 'id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Df with general numerical features\n",
    "features_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET - ONLY NUMERICAL\n",
    "train_keywords_numerical = pd.read_csv('csv_lio/train/keywords_numerical_features - Copy.csv')\n",
    "train_keywords_numerical = train_keywords_numerical.drop(columns = ['Unnamed: 0','target'], axis = 1)\n",
    "train_text_numerical = pd.read_csv('csv_lio/train/text_general_numerical_features_train - Copy.csv')\n",
    "train_text_numerical = train_text_numerical.drop(columns = ['Unnamed: 0','target','#silabas'])\n",
    "train_hashtag_numerical = pd.read_csv('csv_lio/train/features_hashtags_numerical.csv')\n",
    "train_hashtag_numerical  =train_hashtag_numerical.drop(columns = ['Unnamed: 0','target'])\n",
    "train_links_numerical = pd.read_csv('csv_lio/train/features_links_numerical.csv')\n",
    "train_links_numerical = train_links_numerical.drop(columns = ['id.1','target'])\n",
    "train_new_use = train.drop(columns = ['keyword','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.merge(train_new_use,train_text_numerical, on = 'id', how = 'left')\n",
    "features_train = pd.merge(features_train,train_keywords_numerical, on = 'id', how = 'left')\n",
    "features_train = pd.merge(features_train,train_hashtag_numerical, on = 'id', how = 'left')\n",
    "features_train = pd.merge(features_train,train_links_numerical, on = 'id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN: Df with general numerical features\n",
    "features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) b) Transformer-creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text transformer: it selects a single column from the data frame to perform additional transformations on\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "# Value transformer:  it selects a single column from the data frame to perform additional transformations on\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) d) Now, a PIPELINE for each feature in the df is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TEXT -this pipeline consists of \"selecting\" and then \"bow-ing\" a column.\n",
    "bow_pipeline = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('bow',CountVectorizer(analyzer='word',binary=True,ngram_range=(1,2)))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TEXT -  this pipeline consists of \"selecting\" and then \"td_idf-ing\" a column.\n",
    "tfidf_pipeline = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer( ))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TEXT -  this pipeline consists of \"selecting\" and then \"td_idf-ing\" a column.\n",
    "tfidf2_pipeline = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(use_idf=True,smooth_idf=True,ngram_range=(1,1),stop_words='english'))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TEXT -  this pipeline consists of \"selecting\" and then \"td_idf-ing\" a column.\n",
    "tfidf3_pipeline = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(use_idf=True,smooth_idf=True,ngram_range=(1,2),stop_words='english'))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TEXT -  this pipeline consists of \"selecting\" and then \"td_idf-ing\" a column.\n",
    "tfidf4_pipeline = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(use_idf=True,smooth_idf=True,ngram_range=(1,2)))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_pipeline_creator(df_train):\n",
    "    #df.columns.tolist()\n",
    "    feature_names = df_train.columns.tolist()\n",
    "    pipelines = []\n",
    "    for feature_name in feature_names:\n",
    "        current_pipeline =  Pipeline([\n",
    "                            ('selector', NumberSelector(key=feature_name)),\n",
    "                            #('standard', StandardScaler())\n",
    "                        ])\n",
    "        pipelines.append(current_pipeline)\n",
    "        \n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_numerical_pipelines contains all pipelines for all numerical features. To see the correspondance between the array and the\n",
    "# corresponding feature, get the column names of X_train_numerical df\n",
    "train_numerical = features_train.iloc[:,3:31]\n",
    "list_numerical_pipelines = numerical_pipeline_creator(train_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = train_numerical.columns.tolist()\n",
    "for feature_name in feature_names: \n",
    "    print(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines according to feature name\n",
    "cant_palabras= list_numerical_pipelines[0]\n",
    "cant_palabras_unicas= list_numerical_pipelines[1]\n",
    "cant_caracteres= list_numerical_pipelines[2]\n",
    "cant_stopwords= list_numerical_pipelines[3]\n",
    "cant_puntuacion= list_numerical_pipelines[4]\n",
    "cant_capitalize= list_numerical_pipelines[5]\n",
    "cant_mayusculas= list_numerical_pipelines[6]\n",
    "promedio_len_word= list_numerical_pipelines[7]\n",
    "cant_caracteres_especiales= list_numerical_pipelines[8]\n",
    "cant_palabras_binned= list_numerical_pipelines[9]\n",
    "cant_palabras_unicas_binned= list_numerical_pipelines[10]\n",
    "cant_caracteres_binned= list_numerical_pipelines[11]\n",
    "cant_stopwords_binned= list_numerical_pipelines[12]\n",
    "cant_puntuacion_binned= list_numerical_pipelines[13]\n",
    "cant_capitalize_binned= list_numerical_pipelines[14]\n",
    "cant_mayusculas_binned= list_numerical_pipelines[15]\n",
    "cant_silabas_binned= list_numerical_pipelines[16]\n",
    "cant_caracteres_especiales_binned= list_numerical_pipelines[17]\n",
    "text_contains_keyword= list_numerical_pipelines[18]\n",
    "has_keyword= list_numerical_pipelines[19]\n",
    "keywords_quantity= list_numerical_pipelines[20]\n",
    "keywords_mean= list_numerical_pipelines[21]\n",
    "keyword_is_hashtag= list_numerical_pipelines[22]\n",
    "keyword_frequency= list_numerical_pipelines[23]\n",
    "cant_hashtags= list_numerical_pipelines[24]\n",
    "has_hashtag= list_numerical_pipelines[25]\n",
    "links_cant= list_numerical_pipelines[26]\n",
    "cant_failed_links= list_numerical_pipelines[27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) a) Use of FeatureUnion to put features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: Each line is a new feature, so if you wanna add a new feature to the analysis, simply remove the hashtag and run it\n",
    "# again.\n",
    "# If new feture which were not created are to be added, create first and then add it to this group in a new line\n",
    "feats = FeatureUnion([                  ('cant_palabras', cant_palabras), \n",
    "                                     #   ('cant_palabras_unicas', cant_palabras_unicas),\n",
    "                                     #   ('cant_caracteres', cant_caracteres),\n",
    "                                     #   ('cant_stopwords', cant_stopwords),\n",
    "                                     #   ('cant_puntuacion', cant_puntuacion),\n",
    "                                     #   ('cant_capitalize', cant_capitalize),\n",
    "                                     #   ('cant_mayusculas',cant_mayusculas),\n",
    "                                     #   ('promedio_len_word',promedio_len_word),\n",
    "                                     #   ('cant_caracteres_especiales',cant_caracteres_especiales),\n",
    "                                     #   ('cant_palabras_binned',cant_palabras_binned),\n",
    "                                     #   ('cant_palabras_unicas_binned',cant_palabras_unicas_binned),\n",
    "                                     #   ('cant_caracteres_binned',cant_caracteres_binned),\n",
    "                                     #   ('cant_stopwords_binned',cant_stopwords_binned),\n",
    "                                     #   ('cant_puntuacion_binned',cant_puntuacion_binned),\n",
    "                                     #   ('cant_capitalize_binned',cant_capitalize_binned),\n",
    "                                     #   ('cant_mayusculas_binned',cant_mayusculas_binned),\n",
    "                                     #   ('cant_silabas_binned',cant_silabas_binned),\n",
    "                                     #   ('cant_caracteres_especiales_binned',cant_caracteres_especiales_binned),\n",
    "                                     #   ('text_contains_keyword',text_contains_keyword),\n",
    "                                     #   ('has_keyword',has_keyword),\n",
    "                                     #   ('keywords_quantity',keywords_quantity),\n",
    "                                     #   ('keywords_mean',keywords_mean),\n",
    "                                     #   ('keyword_is_hashtag',keyword_is_hashtag),\n",
    "                                     #   ('keyword_frequency',keyword_frequency),\n",
    "                                     #   ('cant_hashtags',cant_hashtags),\n",
    "                                     #   ('has_hashtag',has_hashtag),\n",
    "                                        ('links_cant',links_cant),\n",
    "                                     #   ('cant_failed_links',cant_failed_links),\n",
    "                                     #   ('text_pipeline',text_pipeline)\n",
    "                                        ('bow', bow_pipeline),\n",
    "                                        ('tfidf', tfidf_pipeline),\n",
    "                                     #   ('tfidf2', tfidf2_pipeline),\n",
    "                                     #   ('tfidf3', tfidf3_pipeline),\n",
    "                                     #   ('tfidf4', tfidf4_pipeline),\n",
    "                     ])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processed = feature_processing.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) b) Dani's junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This funtion saves the value of the hyperparameters and the score of the validation train set \n",
    "def save_prediction_accuracy(modelo,x_train,y_train,x_test,y_test):\n",
    "    clf = modelo\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    with open(\"prediction_history.csv\", \"a\") as myfile:\n",
    "        #round(clf.score(X_train, y_train)*100)\n",
    "        train_prediction = str((clf.score(X_train, y_train)*100))\n",
    "        print('Score para x_train: '+ train_prediction)\n",
    "        test_prediction = str((accuracy_score(y_test,predictions)*100))\n",
    "        print('Score para x_test: '+ test_prediction)\n",
    "        params = str(modelo)\n",
    "        print('Hiperparametros: '+ str(modelo))\n",
    "        myfile.write(params+','+test_prediction+\",\"+train_prediction+\",\"+str(datetime.datetime.now())+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(model, test, name):\n",
    "    predicted = model.predict(test)\n",
    "    sample_submission.target = predicted\n",
    "    sample_submission.to_csv('predictions/'+name+'.csv', index=None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('original_data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = features_train.target\n",
    "X = features_train.drop(columns=['target','id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(train_vec,y,test_size=0.2,random_state=2020)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Estimación sobre modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(model,X_train,y_train,X_test,y_test):    \n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions=clf.predict(X_test)\n",
    "    confusion_matrix(y_test,predictions)\n",
    "    conf = metrics.confusion_matrix(y_test, predictions)\n",
    "    conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    print_cm(conf, ['true','false'])\n",
    "    print('-'*50)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print('-'*50)\n",
    "    print(\"{}\" .format(model))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on training set:{}%'.format(round(clf.score(X_train, y_train)*100)))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test,predictions)*100)))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies different ESTIMATORS -with specific or default hyperparameters- to a set of features\n",
    "def runClassifiers(features, X_train, y_train, X_test,y_test):\n",
    "    classifiers = [LogisticRegression(),MultinomialNB(alpha=1),DecisionTreeClassifier(),RandomForestClassifier(),\n",
    "                   SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, random_state=42, max_iter=5, tol=None),\n",
    "                   SGDClassifier(loss='log', penalty='l2', alpha=0.001, random_state=42, max_iter=5, tol=None),\n",
    "                   KNeighborsClassifier(n_neighbors=7, weights = 'distance',p = 1)\n",
    "                 ]\n",
    "\n",
    "    classifier_names = ['Logistic Regression','MultinomialNB','Decision-Tree Classifier',\n",
    "                       'Random-Forest Classifier','SGDC - svc - Classifier','SGDC - lr - Classifier','K-Neighbors Classifier']\n",
    "    i=-1\n",
    "    for classifier in classifiers:\n",
    "        i = i + 1\n",
    "        clf = Pipeline([\n",
    "            ('features', features),\n",
    "            ('actualClassifier', classifier )\n",
    "        ])\n",
    "        fit_and_predict(clf,X_train,y_train,X_test,y_test)\n",
    "        #clf.fit(X_train, y_train)\n",
    "        #docs_test = X_test\n",
    "        #predicted = clf.predict(docs_test)\n",
    "        #print(classifier_names[i],np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probando de todo\n",
    "runClassifiers(feats,X_train.fillna(0),y_train,X_test.fillna(0),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With bow_pipeline\n",
    "runClassifiers(feats,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With bow_pipeline adnds\n",
    "runClassifiers(feats,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With tdidf_pipeline , default values\n",
    "runClassifiers(feats,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With tdidf2_pipeline\n",
    "runClassifiers(feats,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With tdidf3_pipeline\n",
    "runClassifiers(feats,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With tdidf4_pipeline\n",
    "runClassifiers(feats,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With tdidf_pipeline and bow_pipeline\n",
    "runClassifiers(feats,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación con Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "seed=2020\n",
    "kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    " \n",
    "def acc_cross_val(classifier, feats, X):\n",
    "    # Initialize the accuracy of the models to blank list. The accuracy of each model will be appended to this list\n",
    "    accuracy_model = []\n",
    "    # Iterate over each train-test split\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split train-test\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Train the model\n",
    "        clf = Pipeline([\n",
    "            ('features', feats),\n",
    "            ('actualClassifier', classifier)\n",
    "        ])\n",
    "        model = clf.fit(X_train, y_train)\n",
    "        # Append to accuracy_model the accuracy of the model\n",
    "        accuracy_model.append(accuracy_score(y_test, model.predict(X_test), normalize=True)*100)\n",
    "\n",
    "    # Print the accuracy    \n",
    "    print(accuracy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(),MultinomialNB(alpha=1),DecisionTreeClassifier(),RandomForestClassifier(),\n",
    "                   SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, random_state=42, max_iter=5, tol=None),\n",
    "                   SGDClassifier(loss='log', penalty='l2', alpha=0.001, random_state=42, max_iter=5, tol=None),\n",
    "                   KNeighborsClassifier(n_neighbors=7, weights = 'distance',p = 1)\n",
    "                 ]\n",
    "\n",
    "classifier_names = ['Logistic Regression','MultinomialNB','Decision-Tree Classifier',\n",
    "                       'Random-Forest Classifier','SGDC - svc - Classifier','SGDC - lr - Classifier','K-Neighbors Classifier']\n",
    "\n",
    "i = -1\n",
    "for classifier in classifiers:\n",
    "    i = i+1\n",
    "    print(classifier_names[i])\n",
    "    acc_cross_val(classifier, feats, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('multinomialNB', MultinomialNB(alpha=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_prediction_accuracy(model_nb,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/nb_model.sav'\n",
    "pickle.dump(model_nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_lr = LogisticRegression()\n",
    "model_lr = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('logisticReg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 96.00%.\n"
     ]
    }
   ],
   "source": [
    "save_prediction_accuracy(model_lr,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/lr_model.sav'\n",
    "pickle.dump(model_lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('decisionTree', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_dt,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/dt_model.sav'\n",
    "pickle.dump(model_dt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, weights = 'distance',p = 1)\n",
    "model_knn = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('KNN', knn)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_knn,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/knn_model.sav'\n",
    "pickle.dump(model_knn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muy lento\n",
    "'''mlp = MLPClassifier()\n",
    "model_mlp = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('MLP', mlp)\n",
    "])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_prediction_accuracy(model_mlp,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_mlp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'models/mlp_model.sav'\n",
    "#pickle.dump(model_mlp, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd_svc = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('sgd', SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, random_state=42, max_iter=5, tol=None))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_sgd_svc,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd_svc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/sgd_model_svc.sav'\n",
    "pickle.dump(model_sgd_svc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd_lr = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('sgd', SGDClassifier(loss='log', penalty='l2', alpha=0.001, random_state=42, max_iter=5, tol=None))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_sgd_lr,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd_lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/sgd_model_lr.sav'\n",
    "pickle.dump(model_sgd_lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensambles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 1ro con todos los modelos\n",
    "vote  = VotingClassifier(estimators=[('lr', model_lr), ('nb', model_nb), ('dt', model_dt),\n",
    "                                     ('knn', model_knn), ('sgd_lr', model_sgd_lr), ('sgd_svc', model_sgd_svc)], \n",
    "                          voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(vote,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = StackingClassifier(classifiers=[model_nb,model_lr],\n",
    "                          meta_classifier=LogisticRegression()) #,model_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(sclf,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(random_state = 1, n_estimators=100)\n",
    "\n",
    "model_ada = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('adaboost', ada)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_ada,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('xgboost', XGBClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_xgb,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('lgbm', lgb.LGBMClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_lgbm,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model_rf,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':(10,25),\n",
    "    'min_samples_split':(2,20),\n",
    "    'max_features':(0.1,0.999)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo saque del notebook viejo asi que los valores estan dudososs\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV # Number of trees in random forest\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 20)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = list(np.linspace(1, 300, 10, endpoint=True))\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = np.linspace(0.0, 1.0, 10, endpoint=True)\n",
    "                              \n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = np.arange(0.0, 0.5,0.05)\n",
    "                              \n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "                              \n",
    "random_grid = {\"criterion\": [\"entropy\",\"gini\"],\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_eval(n_estimators, min_samples_split, max_features):\n",
    "    rfc = RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "                               min_samples_split=int(min_samples_split),\n",
    "                               max_features=min(max_features, 0.999),\n",
    "                               random_state=2,\n",
    "                               n_jobs=-1)\n",
    "    rf= Pipeline([\n",
    "        ('features', feats),\n",
    "        ('randomforest', rfc )\n",
    "        ])\n",
    "    rf.fit(X_train,y_train)\n",
    "    return (accuracy_score(y_test,rf.predict(X_test))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfBO = BayesianOptimization(rf_eval, {\n",
    "    'n_estimators':(10,25),\n",
    "    'min_samples_split':(2,20),\n",
    "    'max_features':(0.1,0.999)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfBO.maximize(init_points=2,n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfBO.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT ON TEST AND SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"model\" with the final model name\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"model\" with the final model name and name the file (dont forget the timestamp!)\n",
    "save_prediction(model, test.text, 'some_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizacion del texto con Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 300)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "url = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url,binary=True)\n",
    "data = pd.DataFrame()\n",
    "for frase in train.text:\n",
    "    temp = pd.DataFrame()\n",
    "    for word in frase.split(' '):\n",
    "        try:\n",
    "            word_vec = embeddings[word]\n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "    mean = temp.mean()\n",
    "    data = data.append(mean, ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5700"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = train['target']\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6075, 300), (1519, 300), (6075,), (1519,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data.drop('target',axis=1), data['target'], test_size=0.2, random_state=1)\n",
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9fce0673cc39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEnsemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomTreesEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m from ..tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n\u001b[0m\u001b[1;32m     57\u001b[0m                     ExtraTreeClassifier, ExtraTreeRegressor)\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOUBLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDecisionTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_criterion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDepthFirstTreeBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_criterion.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._criterion\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_splitter.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._splitter\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_tree.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._tree\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kde\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKernelDensity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_lof\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLocalOutlierFactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_nca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeighborhoodComponentsAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVALID_METRICS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALID_METRICS_SPARSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/_nca.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdict_learning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdict_learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/dict_learning.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# mypy error: Module X has no attribute y (typically for C extensions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_dict_learning\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pep562\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPep562\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_raise_dep_warning_if_not_pytest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model=AdaBoostClassifier(n_estimators=800, random_state=1)\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "predictions = model.predict(test_X)\n",
    "accuracy = round(accuracy_score(test_y,predictions)*100)\n",
    "print('Model Performance')\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 78.00%.\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "predictions = model.predict(test_X)\n",
    "accuracy = round(accuracy_score(test_y,predictions)*100)\n",
    "print('Model Performance')\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 78.00%.\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "predictions = model.predict(test_X)\n",
    "accuracy = round(accuracy_score(test_y,predictions)*100)\n",
    "print('Model Performance')\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
