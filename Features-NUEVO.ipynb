{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, os, re, string, collections\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  imageio import imread\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import multidict as multidict #pip install multidict\n",
    "import itertools\n",
    "#import csvtomd\n",
    "import re\n",
    "from collections import Counter\n",
    "#%matplotlib inline\n",
    "import spacy\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('original_data/train.csv')\n",
    "test  = pd.read_csv('original_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253\n"
     ]
    }
   ],
   "source": [
    "def getStopwordsList():\n",
    "    fileNamesList = [\"texts/99webtools.txt\", \"texts/atire_ncbi.txt\", \"texts/atire_puurula.txt\", \"texts/azure.txt\", \n",
    "                 \"texts/bbalet.txt\", \n",
    "                 \"texts/bow_short.txt\", \"texts/choi_2000naacl.txt\", \"texts/cook1988_function_words.txt\", \n",
    "                 \"texts/corenlp_acronym.txt\", \n",
    "                 \"texts/corenlp_hardcoded.txt\", \"texts/corenlp_stopwords.txt\", \"texts/datasciencedojo.txt\", \n",
    "                 \"texts/deeplearning4j.txt\", \n",
    "                 \"texts/dkpro.txt\", \"texts/mongodb.txt\", \"texts/galago_inquery.txt\", \"texts/gate_keyphrase.txt\", \n",
    "                 \"texts/gensim.txt\", \n",
    "                 \"texts/glasgow_stop_words.txt\", \"texts/indri.txt\", \"texts/kevinbouge.txt\", \"texts/lexisnexis.txt\",\n",
    "                 \"texts/lingpipe.txt\", \n",
    "                 \"texts/mallet.txt\", \"texts/mysql_innodb.txt\", \"texts/mysql_myisam.txt\", \"texts/galago_rmstop.txt\", \n",
    "                 \"texts/atire_ncbi.txt\", \n",
    "                 \"texts/galago_rmstop.txt\", \"texts/nltk.txt\", \"texts/okapiframework.txt\", \"texts/okapi_cacm_expanded.txt\", \n",
    "                 \"texts/onix.txt\", \n",
    "                 \"texts/ovid.txt\", \"texts/postgresql.txt\", \"texts/pubmed.txt\", \"texts/quanteda.txt\", \"texts/r_tm.txt\", \n",
    "                 \"texts/ranksnl_large.txt\", \n",
    "                 \"texts/reuters_wos.txt\", \"texts/rouge_155.txt\", \"texts/scikitlearn.txt\", \"texts/smart.txt\", \n",
    "                 \"texts/snowball_expanded.txt\", \n",
    "                 \"texts/spacy.txt\", \"texts/spark_mllib.txt\", \"texts/sphinx_mirasvit.txt\", \"texts/t101_minimal.txt\", \n",
    "                 \"texts/taporware.txt\", \n",
    "                 \"texts/terrier.txt\", \"texts/tonybsk_1.txt\", \"texts/tonybsk_6.txt\", \"texts/voyant_taporware.txt\", \n",
    "                 \"texts/weka.txt\", \n",
    "                 \"texts/xapian.txt\", \"texts/xpo6.txt\", \"texts/zettair.txt\"]\n",
    "    stopwordsList = []\n",
    "    for fileName in fileNamesList:\n",
    "        file = open(fileName, \"r\", encoding=\"utf8\")\n",
    "        for line in file:\n",
    "            stripped_line = line. strip()\n",
    "            line_list = stripped_line\n",
    "            if line_list not in stopwordsList:\n",
    "                stopwordsList.append(line_list)\n",
    "        file.close()\n",
    "    return stopwordsList\n",
    "\n",
    "stopwordsList = getStopwordsList()\n",
    "\n",
    "print(len(stopwordsList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_strict(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\d+', '')\n",
    "    invalid_chars = ['#','|','@','!','?','-','_','[',']','%','&',':','.',',',\"''\",'/','https','(','//t',')','http',\n",
    "                 ';','\\'']\n",
    "    for char in invalid_chars:\n",
    "        if char in text:\n",
    "            text = text.replace(char,' ')\n",
    "    #removes url and tags\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \" \",  text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES AUXILIARES\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def prettify2(word_list):\n",
    "    wl = []\n",
    "    for word in word_list:\n",
    "        word = re.sub(r'[^A-Za-z]', \" \",  word)\n",
    "        word = re.sub(r'\\b\\w\\b',' ', word)\n",
    "        word = re.sub(r'\\s+', ' ', word)\n",
    "        wl.append(word.strip())\n",
    "    return wl\n",
    "\n",
    "def prettify(word):\n",
    "    word = re.sub(r'[^A-Za-z]', \" \",  word)\n",
    "    word = re.sub(r'\\b\\w\\b',' ', word)\n",
    "    word = re.sub(r'\\s+', ' ', word)\n",
    "    return word\n",
    "\n",
    "def lemmatize_text2(word_list):\n",
    "    text = ' '\n",
    "    text = text.join(word_list)\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "def getText(word_list):\n",
    "    text = ' '\n",
    "    text = text.join(word_list)\n",
    "    return text\n",
    "\n",
    "def word_finder(x):\n",
    "    x = set(x)\n",
    "    return list(unique_frequent_words.intersection(x))\n",
    "\n",
    "def cleanStopwords(tweet):\n",
    "    tweet = tweet.split(' ')\n",
    "    cleanTweet=\"\"\n",
    "    for word in tweet:\n",
    "        if word not in stopwordsList:\n",
    "            cleanTweet = cleanTweet + \" \" + word\n",
    "    return cleanTweet\n",
    "\n",
    "\n",
    "#tweet = \"Our Deeds are the Reason of this #earthquake M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text = train[\"text\"].apply(lambda x: clean_text_strict(x))\n",
    "test.text = test[\"text\"].apply(lambda x: clean_text_strict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this  earthquake m...\n",
       "1                  forest fire near la ronge sask  canada\n",
       "2       all residents asked to  shelter in place  are ...\n",
       "3       13 000 people receive  wildfires evacuation or...\n",
       "4       just got sent this photo from ruby  alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609     aria ahrary  thetawniest the out of control w...\n",
       "7610    m1 94  01 04 utc  5km s of volcano hawaii     ...\n",
       "7611    police investigating after an e bike collided ...\n",
       "7612    the latest  more homes razed by northern calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text = train.text.apply(prettify)\n",
    "test.text = test.text.apply(prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this earthquake ma...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents asked to shelter in place are be...\n",
       "3        people receive wildfires evacuation orders in...\n",
       "4       just got sent this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding bridge collapse into ...\n",
       "7609     aria ahrary thetawniest the out of control wi...\n",
       "7610               utc km of volcano hawaii co zdtoyd ebj\n",
       "7611    police investigating after an bike collided wi...\n",
       "7612    the latest more homes razed by northern califo...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def lemmatize_text_v2(text):\n",
    "    if text == '' or pd.isnull(text):\n",
    "        return text\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "train.text = train.text.apply(lemmatize_text_v2)\n",
    "test.text  = test.text.apply(lemmatize_text_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -PRON- deed be the reason of this earthquake m...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all resident ask to shelter in place be be not...\n",
       "3         people receive wildfire evacuation order in ...\n",
       "4       just get send this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7608    two giant crane hold bridge collapse into near...\n",
       "7609      aria ahrary thetawniest the out of control w...\n",
       "7610               utc km of volcano hawaii co zdtoyd ebj\n",
       "7611    police investigate after an bike collide with ...\n",
       "7612    the late more home raze by northern california...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text = train.text.apply(cleanStopwords)\n",
    "test.text  = test.text.apply(cleanStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          terrible crash\n",
       "1                                    earthquake city safe\n",
       "2          forest fire spot pond goose flee street -PRON-\n",
       "3                    apocalypse lighting spokane wildfire\n",
       "4                      typhoon soudelor kill china taiwan\n",
       "                              ...                        \n",
       "3258     earthquake safety los angeles safety fastener...\n",
       "3259     storm ri hurricane -PRON- city hit -PRON- yar...\n",
       "3260                  green derailment chicago utbxlcbiuy\n",
       "3261                     meg hazardous outlook hwo rbqjhn\n",
       "3262       cityofcalgary activate -PRON- municipal eme...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer=CountVectorizer(analyzer='word',binary=True)\n",
    "count_vectorizer.fit(train.text)\n",
    "\n",
    "train_vec = count_vectorizer.fit_transform(train.text)\n",
    "test_vec = count_vectorizer.transform(test.text)\n",
    "\n",
    "print(train_vec[7].todense())\n",
    "print(test_vec[7].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7613x19203 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 59234 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          terrible crash\n",
       "1                                    earthquake city safe\n",
       "2          forest fire spot pond goose flee street -PRON-\n",
       "3                    apocalypse lighting spokane wildfire\n",
       "4                      typhoon soudelor kill china taiwan\n",
       "                              ...                        \n",
       "3258     earthquake safety los angeles safety fastener...\n",
       "3259     storm ri hurricane -PRON- city hit -PRON- yar...\n",
       "3260                  green derailment chicago utbxlcbiuy\n",
       "3261                     meg hazardous outlook hwo rbqjhn\n",
       "3262       cityofcalgary activate -PRON- municipal eme...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_vec,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(train_vec,y,test_size=0.2,random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 91.00%.\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "accuracy = round(accuracy_score(y_test,predictions)*100)\n",
    "print('Model Performance')\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para x_train: 0.9254515599343186\n",
      "Score para x_test: 0.9146421536441235\n",
      "Hiperparametros: MultinomialNB(alpha=1)\n"
     ]
    }
   ],
   "source": [
    "with open(\"prediction_history.csv\", \"a\") as myfile:\n",
    "    predicted = model.predict(X_train)\n",
    "    train_prediction = str(metrics.accuracy_score(y_train,predicted))\n",
    "    print('Score para x_train: '+ train_prediction)\n",
    "    predicted = model.predict(X_test)\n",
    "    test_prediction = str(metrics.accuracy_score(y_test,predicted))\n",
    "    print('Score para x_test: '+ test_prediction)\n",
    "    params = str(model)\n",
    "    print('Hiperparametros: '+ str(model))\n",
    "    myfile.write(params+','+test_prediction+\",\"+train_prediction+\",\"+str(datetime.datetime.now())+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission=pd.read_csv('original_data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.target= predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('just_text_MB_'+str(datetime.datetime.now())+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
