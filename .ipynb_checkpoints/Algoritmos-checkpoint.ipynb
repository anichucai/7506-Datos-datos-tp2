{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A partir de features elegidos con DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>K_fire</th>\n",
       "      <th>K_bomb</th>\n",
       "      <th>K_storm</th>\n",
       "      <th>K_typhoon</th>\n",
       "      <th>K_debris</th>\n",
       "      <th>K_derail</th>\n",
       "      <th>K_police</th>\n",
       "      <th>K_flood</th>\n",
       "      <th>K_wreckage</th>\n",
       "      <th>...</th>\n",
       "      <th>tag_people_y</th>\n",
       "      <th>tag_injured</th>\n",
       "      <th>tag_disaster_y</th>\n",
       "      <th>tag_sinkhole_y</th>\n",
       "      <th>tag_casualty_y</th>\n",
       "      <th>tag_evacuation</th>\n",
       "      <th>tag_drought_y</th>\n",
       "      <th>tag_investigator</th>\n",
       "      <th>tag_rioting</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.676074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.557733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 738 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  K_fire  K_bomb  K_storm  K_typhoon  K_debris  K_derail  K_police  \\\n",
       "0   1       0       0        0          0         0         0         0   \n",
       "1   4       1       0        0          0         0         0         0   \n",
       "2   5       0       0        0          0         0         0         0   \n",
       "3   6       1       0        0          0         0         0         0   \n",
       "4   7       1       0        0          0         0         0         0   \n",
       "\n",
       "   K_flood  K_wreckage  ...  tag_people_y  tag_injured  tag_disaster_y  \\\n",
       "0        0           0  ...      0.000000          0.0             0.0   \n",
       "1        0           0  ...      0.000000          0.0             0.0   \n",
       "2        0           0  ...      0.000000          0.0             0.0   \n",
       "3        0           0  ...      0.435025          0.0             0.0   \n",
       "4        0           0  ...      0.000000          0.0             0.0   \n",
       "\n",
       "   tag_sinkhole_y  tag_casualty_y  tag_evacuation  tag_drought_y  \\\n",
       "0             0.0             0.0        0.000000            0.0   \n",
       "1             0.0             0.0        0.000000            0.0   \n",
       "2             0.0             0.0        0.676074            0.0   \n",
       "3             0.0             0.0        0.557733            0.0   \n",
       "4             0.0             0.0        0.000000            0.0   \n",
       "\n",
       "   tag_investigator  tag_rioting  target  \n",
       "0               0.0          0.0       1  \n",
       "1               0.0          0.0       1  \n",
       "2               0.0          0.0       1  \n",
       "3               0.0          0.0       1  \n",
       "4               0.0          0.0       1  \n",
       "\n",
       "[5 rows x 738 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = pd.read_csv('features/feature_selection/features_decition_tree2.csv').drop(columns=['Unnamed: 0'])\n",
    "train = pd.read_csv(\"original_data/train.csv\", dtype = {\"target\": int}, encoding='UTF_8')\n",
    "train = train[['id','target']]\n",
    "train_features = train_features.merge(train, on='id')\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_numerical = pd.read_csv('features/test/features_keywords_numerical.csv').drop(columns=['Unnamed: 0'])\n",
    "text_numerical = pd.read_csv('features/test/features_text_numerical.csv').drop(columns=['Unnamed: 0'])\n",
    "location_in_text_numerical = pd.read_csv('features/test/features_location_in_text_numerical.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_country</th>\n",
       "      <th>has_city</th>\n",
       "      <th>has_county</th>\n",
       "      <th>has_state</th>\n",
       "      <th>has_continent</th>\n",
       "      <th>#palabras</th>\n",
       "      <th>#palabras_unicas</th>\n",
       "      <th>#caracteres</th>\n",
       "      <th>#stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>#capitalize_binned</th>\n",
       "      <th>#mayusculas_binned</th>\n",
       "      <th>#silabas_binned</th>\n",
       "      <th>#caracteres_especiales_binned</th>\n",
       "      <th>text_contains_keyword</th>\n",
       "      <th>has_keyword</th>\n",
       "      <th>keywords_quantity</th>\n",
       "      <th>keywords_mean</th>\n",
       "      <th>keyword_is_hashtag</th>\n",
       "      <th>keyword_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.183784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>98</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  has_country  has_city  has_county  has_state  has_continent  #palabras  \\\n",
       "0   0            0         0           0          0              0          6   \n",
       "1   2            0         0           0          0              0          9   \n",
       "2   3            0         0           0          0              0         19   \n",
       "3   9            0         0           0          0              0          4   \n",
       "4  11            1         0           0          0              1          8   \n",
       "\n",
       "   #palabras_unicas  #caracteres  #stopwords  ...  #capitalize_binned  \\\n",
       "0                 6           36           1  ...                   0   \n",
       "1                 9           66           2  ...                   0   \n",
       "2                19           98           8  ...                   0   \n",
       "3                 4           42           0  ...                   1   \n",
       "4                 8           47           2  ...                   1   \n",
       "\n",
       "   #mayusculas_binned  #silabas_binned  #caracteres_especiales_binned  \\\n",
       "0                   0                0                              0   \n",
       "1                   0                1                              1   \n",
       "2                   0                1                              0   \n",
       "3                   0                0                              1   \n",
       "4                   0                1                              0   \n",
       "\n",
       "   text_contains_keyword  has_keyword  keywords_quantity  keywords_mean  \\\n",
       "0                      1            1                  1       0.004525   \n",
       "1                      1            1                  1       0.004525   \n",
       "2                      1            1                  1       0.004525   \n",
       "3                      1            1                  1       0.004525   \n",
       "4                      1            1                  1       0.004525   \n",
       "\n",
       "   keyword_is_hashtag  keyword_frequency  \n",
       "0                   0           0.183784  \n",
       "1                   1           0.075676  \n",
       "2                   0           0.075676  \n",
       "3                   0           0.102703  \n",
       "4                   0           0.070270  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = location_in_text_numerical.merge(text_numerical, on='id')\n",
    "test_features = test_features.merge(keywords_numerical, on='id').fillna(0)\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_csv('feature_selection_on_test_DT').drop(columns='Unnamed: 0').fillna(0)\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_accuracy(modelo,x_train,y_train,x_test,y_test):\n",
    "    with open(\"prediction_history.csv\", \"a\") as myfile:\n",
    "        predicted = modelo.predict(x_train)\n",
    "        train_prediction = str(metrics.accuracy_score(y_train,predicted))\n",
    "        print('Score para x_train: '+ train_prediction)\n",
    "        predicted = modelo.predict(x_test)\n",
    "        test_prediction = str(metrics.accuracy_score(y_test,predicted))\n",
    "        print('Score para x_test: '+ test_prediction)\n",
    "        params = str(modelo)\n",
    "        print('Hiperparametros: '+ str(modelo))\n",
    "        myfile.write(params+','+test_prediction+\",\"+train_prediction+\",\"+str(datetime.datetime.now())+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(model, test, name):\n",
    "    df_prediccion = test[['id']].copy()\n",
    "    predicted = model.predict(test.drop(columns='id'))\n",
    "    df_prediccion['target'] = predicted\n",
    "    df_prediccion.to_csv('predictions/'+name+str(datetime.datetime.now())+'.csv', index=None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_features.drop(columns = ['target'])\n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "#Split the CountVector vectorized data into train and test datasets for model training and testing\n",
    "X_train, X_test, y_train, y_test =train_test_split(train_df,train_features.target,test_size=0.2,random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aca hay mas cosas interesantes https://www.ritchieng.com/machine-learning-evaluate-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    #errors = abs(predictions - test_labels)\n",
    "    #mape = 100 * np.mean(errors / test_labels)\n",
    "    #accuracy = 100 - mape\n",
    "    accuracy = round(accuracy_score(test_labels,predictions)*100)\n",
    "    print('Model Performance')\n",
    "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(model,X_train,y_train,X_test,y_test):    \n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions=clf.predict(X_test)\n",
    "    confusion_matrix(y_test,predictions)\n",
    "    conf = metrics.confusion_matrix(y_test, predictions)\n",
    "    conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    print_cm(conf, ['true','false'])\n",
    "    print('-'*50)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print('-'*50)\n",
    "    print(\"{}\" .format(model))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on training set:{}%'.format(round(clf.score(X_train, y_train)*100)))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test,predictions)*100)))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[LogisticRegression(C=1.0, max_iter=1000),SVC(),MultinomialNB(),DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors=5),RandomForestClassifier(),GaussianNB(),SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = X_test.copy(\"id\")\n",
    "X_test = X_test.drop(columns='id')\n",
    "id_train = X_train.copy(\"id\")\n",
    "X_train = X_train.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.3   0.7 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.89      0.82       849\n",
      "           1       0.82      0.66      0.73       674\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.77      0.78      1523\n",
      "weighted avg       0.79      0.79      0.78      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:81%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:79%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.6   0.4 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.84      0.73       849\n",
      "           1       0.67      0.42      0.52       674\n",
      "\n",
      "    accuracy                           0.65      1523\n",
      "   macro avg       0.66      0.63      0.62      1523\n",
      "weighted avg       0.66      0.65      0.64      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:66%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:65%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.86      0.78       849\n",
      "           1       0.76      0.58      0.66       674\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.74      0.72      0.72      1523\n",
      "weighted avg       0.74      0.73      0.73      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:75%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:73%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.7   0.3 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       849\n",
      "           1       0.62      0.65      0.63       674\n",
      "\n",
      "    accuracy                           0.67      1523\n",
      "   macro avg       0.66      0.67      0.67      1523\n",
      "weighted avg       0.67      0.67      0.67      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:99%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:67%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.5   0.5 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72       849\n",
      "           1       0.66      0.45      0.54       674\n",
      "\n",
      "    accuracy                           0.65      1523\n",
      "   macro avg       0.65      0.63      0.63      1523\n",
      "weighted avg       0.65      0.65      0.64      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:76%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:65%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80       849\n",
      "           1       0.80      0.62      0.69       674\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.77      0.74      0.75      1523\n",
      "weighted avg       0.77      0.76      0.75      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:99%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:76%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   1.0   0.0 \n",
      "    false   0.6   0.4 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.80       849\n",
      "           1       0.92      0.44      0.60       674\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.80      0.70      0.70      1523\n",
      "weighted avg       0.79      0.73      0.71      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:75%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:73%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.7   0.3 \n",
      "    false   0.2   0.8 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.75       849\n",
      "           1       0.68      0.75      0.71       674\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.73      0.73      1523\n",
      "weighted avg       0.74      0.73      0.73      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:75%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:73%\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_predict(model,X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de accuracy con Cross-Validation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict_cross_val(model,X_train,y_train,X_test,y_test):\n",
    "    print(\"{}\" .format(model))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on training set:{}%'.format(round(cross_val_score(model, X_train, y_train, scoring='accuracy', cv = 10).mean()*100)))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on test set:{}%' .format(round(cross_val_score(model, X_test, y_test, scoring='accuracy', cv = 10).mean()*100)))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier on training set:78%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier on test set:76%\n",
      "**************************************************\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:66%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:61%\n",
      "**************************************************\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:74%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:71%\n",
      "**************************************************\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:69%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:66%\n",
      "**************************************************\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:65%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:62%\n",
      "**************************************************\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:77%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:73%\n",
      "**************************************************\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:73%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:72%\n",
      "**************************************************\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:71%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:60%\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    fit_and_predict_cross_val(model,X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': ['entropy', 'gini'], 'n_estimators': [200, 294, 389, 484, 578, 673, 768, 863, 957, 1052, 1147, 1242, 1336, 1431, 1526, 1621, 1715, 1810, 1905, 2000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [1.0, 34.22222222222222, 67.44444444444444, 100.66666666666666, 133.88888888888889, 167.11111111111111, 200.33333333333331, 233.55555555555554, 266.77777777777777, 300.0, None], 'min_samples_split': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
      "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]), 'min_samples_leaf': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45]), 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV # Number of trees in random forest\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 20)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = list(np.linspace(1, 300, 10, endpoint=True))\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = np.linspace(0.0, 1.0, 10, endpoint=True)\n",
    "                              \n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = np.arange(0.0, 0.5,0.05)\n",
    "                              \n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "                              \n",
    "random_grid = {\"criterion\": [\"entropy\",\"gini\"],\n",
    "                'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   42.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                                        'min_samples_leaf': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45]),\n",
       "                                        'min_samples_split': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "                                        'n_estimators': [200, 294, 389, 484,\n",
       "                                                         578, 673, 768, 863,\n",
       "                                                         957, 1052, 1147, 1242,\n",
       "                                                         1336, 1431, 1526, 1621,\n",
       "                                                         1715, 1810, 1905,\n",
       "                                                         2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs=-1)# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 233.55555555555554,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 0.05,\n",
       " 'min_samples_split': 0.3333333333333333,\n",
       " 'n_estimators': 957}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 74.00%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 65.00%.\n"
     ]
    }
   ],
   "source": [
    "best_RF = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_RF, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of -12.16%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=233.55555555555554,\n",
       "                       max_features='sqrt', max_leaf_nodes=None,\n",
       "                       max_samples=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=0.05,\n",
       "                       min_samples_split=0.3333333333333333,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=957,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para x_train: 0.6474548440065682\n",
      "Score para x_test: 0.6480630334865397\n",
      "Hiperparametros: RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=233.55555555555554,\n",
      "                       max_features='sqrt', max_leaf_nodes=None,\n",
      "                       max_samples=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=0.05,\n",
      "                       min_samples_split=0.3333333333333333,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=957,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "save_prediction_accuracy(best_RF, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIGHT GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and testing dataset\n",
    "train_set = lgb.Dataset(data = X_train, label = y_train)\n",
    "test_set = lgb.Dataset(data = X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    }
   ],
   "source": [
    "N_FOLDS = 5\n",
    "MAX_EVALS = 5\n",
    "\n",
    "# Get default hyperparameters\n",
    "model = lgb.LGBMClassifier()\n",
    "default_params = model.get_params()\n",
    "\n",
    "# Remove the number of estimators because we set this to 10000 in the cv call\n",
    "del default_params['n_estimators']\n",
    "\n",
    "# Cross validation with early stopping\n",
    "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = N_FOLDS, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum validation ROC AUC was: 0.83632 with a standard deviation of 0.01245.\n",
      "The optimal number of boosting rounds (estimators) was 94.\n"
     ]
    }
   ],
   "source": [
    "print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8cc8ec6248aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbaseline_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The baseline model scores {:.5f} ROC AUC on the test set.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Optimal number of esimators found in cv\n",
    "model.n_estimators = len(cv_results['auc-mean'])\n",
    "\n",
    "# Train and make predicions with model\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict_proba(X_test)\n",
    "baseline_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_accuracy(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUNING [se me rompe todo cuando quiero ejecutar los search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTO DEBERIA DAR LO MISMO QUE ANTES PORQUE LE PASO LOS PARAMETROS DEFAULT\n",
    "score, params, iteration = objective(default_params, 1)\n",
    "\n",
    "print('The cross-validation ROC AUC was {:.5f}.'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'goss'],\n",
    "    'num_leaves': list(range(20, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "    'is_unbalance': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes for random and grid search\n",
    "random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "\n",
    "grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grid_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_results = grid_search(param_grid)\n",
    "\n",
    "#print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "#print('\\nThe best hyperparameters were:')\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "#grid_search_params = grid_results.loc[0, 'params']\n",
    "\n",
    "# Create, train, test model\n",
    "#model = lgb.LGBMClassifier(**grid_search_params, random_state=42)\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "#preds = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Dataframe for results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                                  index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # Keep searching until reach max evaluations\n",
    "    for i in range(MAX_EVALS):\n",
    "        \n",
    "        # Choose random hyperparameters\n",
    "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "\n",
    "        # Evaluate randomly selected hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "    \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_results = random_search(param_grid)\n",
    "\n",
    "#print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\n",
    "#print('\\nThe best hyperparameters were:')\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(random_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actuals_onehot = pd.get_dummies(y_train).values\n",
    "#false_positive_rate, recall, thresholds = roc_curve(actuals_onehot[0], np.round(predictions)[0])\n",
    "#roc_auc = auc(false_positive_rate, recall)\n",
    "#print(\"AUC score \",roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURONAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "params_MLPC = {'hidden_layer_sizes':[(n,n,n) for n in range(1,30)],\n",
    "                              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "                              'alpha':[1e-06,1e-05,1e-04,1e-03,1e-02,1e-01,1],\n",
    "                              'beta_1':np.arange(0.0,1,0.01),\n",
    "                              'beta_2':np.arange(0.0,1,0.01),\n",
    "                              'early_stopping':[True, False],\n",
    "                              'epsilon':[1e-07,1e-08,1e-06, 1e-09, 1e-010, 1e-11],\n",
    "                              'learning_rate':['constant', 'adaptive'],\n",
    "                              'solver':['adam', 'lbfgs', 'sgd'],\n",
    "                              'validation_fraction':np.arange(0.15,0.5,0.01),\n",
    "                               'max_iter':[200,300,400],\n",
    "                             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                           batch_size='auto', beta_1=0.9,\n",
       "                                           beta_2=0.999, early_stopping=False,\n",
       "                                           epsilon=1e-08,\n",
       "                                           hidden_layer_sizes=(100,),\n",
       "                                           learning_rate='constant',\n",
       "                                           learning_rate_init=0.001,\n",
       "                                           max_fun=15000, max_iter=200,\n",
       "                                           momentum=0.9, n_iter_no_change=10,\n",
       "                                           nesterovs_momentum=True, power_t=0.5,\n",
       "                                           random...\n",
       "                                        'max_iter': [200, 300, 400],\n",
       "                                        'solver': ['adam', 'lbfgs', 'sgd'],\n",
       "                                        'validation_fraction': array([0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22, 0.23, 0.24, 0.25,\n",
       "       0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33, 0.34, 0.35, 0.36,\n",
       "       0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47,\n",
       "       0.48, 0.49])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = MLPClassifier()\n",
    "rf_MLP = RandomizedSearchCV(estimator = rf, param_distributions = params_MLPC, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs=-1)# Fit the random search model\n",
    "rf_MLP.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.001,\n",
       " 'beta_1': 0.67,\n",
       " 'beta_2': 0.75,\n",
       " 'early_stopping': True,\n",
       " 'epsilon': 1e-11,\n",
       " 'hidden_layer_sizes': (28, 28, 28),\n",
       " 'learning_rate': 'constant',\n",
       " 'max_iter': 300,\n",
       " 'solver': 'adam',\n",
       " 'validation_fraction': 0.2600000000000001}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_MLP.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 74.00%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 65.00%.\n"
     ]
    }
   ],
   "source": [
    "best_MLP = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_MLP, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of -12.16%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para x_train: 0.6474548440065682\n",
      "Score para x_test: 0.6480630334865397\n",
      "Hiperparametros: RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=233.55555555555554,\n",
      "                       max_features='sqrt', max_leaf_nodes=None,\n",
      "                       max_samples=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=0.05,\n",
      "                       min_samples_split=0.3333333333333333,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=957,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "save_prediction_accuracy(best_MLP, X_train, y_train, X_test, y_test)\n",
    "save_prediction(best_MLP, test_features,'MLP_selected_w_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC \n",
    "params_GBC = {\n",
    "    'max_features':['auto', 'sqrt', 'log2'],\n",
    "                             'max_leaf_nodes': [None,1,2,3,4,5,6,8],\n",
    "                             'min_weight_fraction_leaf': np.linspace(0.0, 0.5, 1, endpoint=True),\n",
    "                             'learning_rate': np.arange(0.1, 0.5, 0.05),\n",
    "                             'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "                             'min_samples_leaf': np.arange(0.1, 0.5, 5),  \n",
    "                             'max_features' : list(range(1,100)),\n",
    "                             'max_depth': [n for n in range(0,50,5)],\n",
    "                             'n_estimators': [n for n in range(0,1000,10)],\n",
    "                             'subsample': np.arange(0.3, 1,0.1),\n",
    "                             'loss': ['deviance'],\n",
    "                             'warm_start': [True, False],\n",
    "                             'presort': ['auto'],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=GradientBoostingClassifier(ccp_alpha=0.0,\n",
       "                                                        criterion='friedman_mse',\n",
       "                                                        init=None,\n",
       "                                                        learning_rate=0.1,\n",
       "                                                        loss='deviance',\n",
       "                                                        max_depth=3,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_ite...\n",
       "                                        'min_weight_fraction_leaf': array([0.]),\n",
       "                                        'n_estimators': [0, 10, 20, 30, 40, 50,\n",
       "                                                         60, 70, 80, 90, 100,\n",
       "                                                         110, 120, 130, 140,\n",
       "                                                         150, 160, 170, 180,\n",
       "                                                         190, 200, 210, 220,\n",
       "                                                         230, 240, 250, 260,\n",
       "                                                         270, 280, 290, ...],\n",
       "                                        'presort': ['auto'],\n",
       "                                        'subsample': array([0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
       "                                        'warm_start': [True, False]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = GBC()\n",
    "rf_GBC = RandomizedSearchCV(estimator = rf, param_distributions = params_GBC, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "rf_GBC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.25000000000000006,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 35,\n",
       " 'max_features': 52,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 0.1,\n",
       " 'min_samples_split': 0.1,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 190,\n",
       " 'presort': 'auto',\n",
       " 'subsample': 0.8000000000000003,\n",
       " 'warm_start': True}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_GBC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 74.00%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 71.00%.\n"
     ]
    }
   ],
   "source": [
    "best_GBC = rf_GBC.best_estimator_\n",
    "random_accuracy = evaluate(best_GBC, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of -4.05%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para x_train: 0.7394088669950739\n",
      "Score para x_test: 0.7078135259356533\n",
      "Hiperparametros: GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.25000000000000006, loss='deviance',\n",
      "                           max_depth=35, max_features=52, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.1,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=190,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=0.8000000000000003,\n",
      "                           tol=0.0001, validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=True)\n"
     ]
    }
   ],
   "source": [
    "save_prediction_accuracy(best_GBC, X_train, y_train, X_test, y_test)\n",
    "save_prediction(best_GBC, test_features,'GBC_selected_w_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb #conda install -c conda-forge xgboost \n",
    "\n",
    "params_XGB = {\n",
    "                'objective': ['binary:logistic','reg:linear'],\n",
    "                'learning_rate':np.arange(0.1,0.5,0.1),\n",
    "                'n_estimators':np.arange(16,116,15),\n",
    "                'scale_pos_weight':np.arange(2,6,1),\n",
    "                'max_depth':np.arange(4,12,1),'min_child_weight':np.arange(1,10,1),\n",
    "                'gamma':np.arange(0,0.5,0.1),\n",
    "                'subsample':np.arange(0.6,1,0.1),'colsample_bytree':np.arange(0.6,0.91,0.05),\n",
    "                'colsample_bylevel':np.arange(0.6,0.91,0.05),\n",
    "                'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "                'tree_method':['auto', 'exact', 'approx', 'hist', 'gpu_hist']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n...\n",
       "                                        'n_estimators': array([ 16,  31,  46,  61,  76,  91, 106]),\n",
       "                                        'objective': ['binary:logistic',\n",
       "                                                      'reg:linear'],\n",
       "                                        'reg_alpha': [1e-05, 0.01, 0.1, 1, 100],\n",
       "                                        'scale_pos_weight': array([2, 3, 4, 5]),\n",
       "                                        'subsample': array([0.6, 0.7, 0.8, 0.9]),\n",
       "                                        'tree_method': ['auto', 'exact',\n",
       "                                                        'approx', 'hist',\n",
       "                                                        'gpu_hist']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = xgb.XGBClassifier()\n",
    "rf_XGB = RandomizedSearchCV(estimator = rf, param_distributions = params_XGB, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "rf_XGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bylevel': 0.8500000000000002,\n",
       " 'colsample_bytree': 0.8500000000000002,\n",
       " 'gamma': 0.1,\n",
       " 'learning_rate': 0.30000000000000004,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 2,\n",
       " 'n_estimators': 91,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': 0.01,\n",
       " 'scale_pos_weight': 2,\n",
       " 'subsample': 0.7999999999999999,\n",
       " 'tree_method': 'approx'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_XGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 74.00%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 75.00%.\n"
     ]
    }
   ],
   "source": [
    "best_XGB = rf_XGB.best_estimator_\n",
    "random_accuracy = evaluate(best_XGB, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of 1.35%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para x_train: 0.7394088669950739\n",
      "Score para x_test: 0.7078135259356533\n",
      "Hiperparametros: GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.25000000000000006, loss='deviance',\n",
      "                           max_depth=35, max_features=52, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.1,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=190,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=0.8000000000000003,\n",
      "                           tol=0.0001, validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=True)\n"
     ]
    }
   ],
   "source": [
    "save_prediction_accuracy(best_GBC, X_train, y_train, X_test, y_test)\n",
    "save_prediction(best_GBC, test_features,'XGB_selected_w_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_LR = {\n",
    "    'penalty':['l1', 'l2', 'elasticnet'],\n",
    "    'C': np.arange(0.5,0.8,20),\n",
    "    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   24.0s remaining:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   25.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=100,\n",
       "                                                multi_class='auto', n_jobs=None,\n",
       "                                                penalty='l2', random_state=None,\n",
       "                                                solver='lbfgs', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'C': array([0.5]),\n",
       "                                        'multi_class': ['auto', 'ovr',\n",
       "                                                        'multinomial'],\n",
       "                                        'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                                        'solver': ['newton-cg', 'lbfgs',\n",
       "                                                   'liblinear', 'sag',\n",
       "                                                   'saga']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = LogisticRegression()\n",
    "rf_LG = RandomizedSearchCV(estimator = rf, param_distributions = params_LR, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "rf_LG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.5,\n",
       " 'multi_class': 'multinomial',\n",
       " 'penalty': 'l2',\n",
       " 'solver': 'newton-cg'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_LG.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 74.00%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Accuracy = 79.00%.\n"
     ]
    }
   ],
   "source": [
    "best_LG = rf_LG.best_estimator_\n",
    "random_accuracy = evaluate(best_LG, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of 6.76%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para x_train: 0.810016420361248\n",
      "Score para x_test: 0.7931713722915299\n",
      "Hiperparametros: LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "save_prediction_accuracy(best_LG, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "save_prediction(best_RF, test_features,'RF_selected_w_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTGBM\n",
    "save_prediction(model, test_features,'LGBM_selected_w_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORKS\n",
    "save_prediction(best_MLP, test_features,'MLP_selected_w_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "save_prediction(best_LG, test_features,'LG_selected_w_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
