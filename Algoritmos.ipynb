{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#9c8f8f\"> 75.06/95.58 Organización de Datos</span>\n",
    "# <span style=\"color:#9c8f8f\"> Análisis exploratorio: Real or Not? NLP with Disaster Tweets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ALGORITMOS</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_accuracy(modelo,x_train,y_train,x_test,y_test):\n",
    "    with open(\"prediction_history.csv\", \"a\") as myfile:\n",
    "        predicted = modelo.predict(x_train)\n",
    "        train_prediction = str(metrics.accuracy_score(y_train,predicted))\n",
    "        print('Score para x_train: '+ train_prediction)\n",
    "        predicted = modelo.predict(x_test)\n",
    "        test_prediction = str(metrics.accuracy_score(y_test,predicted))\n",
    "        print('Score para x_test: '+ test_prediction)\n",
    "        params = str(modelo)\n",
    "        print('Hiperparametros: '+ str(modelo))\n",
    "        myfile.write(params+','+test_prediction+\",\"+train_prediction+\",\"+str(datetime.datetime.now())+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(model, test, name):\n",
    "    df_prediccion = test[['id']].copy()\n",
    "    predicted = model.predict(test.drop(columns='id'))\n",
    "    df_prediccion['target'] = predicted\n",
    "    df_prediccion.to_csv('predictions/'+name+str(datetime.datetime.now())+'.csv', index=None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    #errors = abs(predictions - test_labels)\n",
    "    #mape = 100 * np.mean(errors / test_labels)\n",
    "    #accuracy = 100 - mape\n",
    "    accuracy = round(accuracy_score(test_labels,predictions)*100)\n",
    "    print('Model Performance')\n",
    "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(model,X_train,y_train,X_test,y_test):    \n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions=clf.predict(X_test)\n",
    "    confusion_matrix(y_test,predictions)\n",
    "    conf = metrics.confusion_matrix(y_test, predictions)\n",
    "    conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    print_cm(conf, ['true','false'])\n",
    "    print('-'*50)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print('-'*50)\n",
    "    print(\"{}\" .format(model))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on training set:{}%'.format(round(clf.score(X_train, y_train)*100)))\n",
    "    print('-'*50)\n",
    "    print('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test,predictions)*100)))\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_models_sin_tune(X_test,X_train,y_test, y_train, models):\n",
    "    \n",
    "    for model in models:\n",
    "        fit_and_predict(model,X_train, y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpiar features de df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_train = {\"id\": np.int32, \"keyword\": \"category\", \"target\" : int}\n",
    "train = pd.read_csv(\"original_data/train.csv\", dtype = dtype_train, encoding='UTF_8')\n",
    "\n",
    "target_train = train[[\"id\",\"target\"]]\n",
    "    \n",
    "def get_clean_values(df):\n",
    "    \n",
    "    if(\"target\" in list(df.columns)):\n",
    "        del df[\"target\"]\n",
    "    df = df.merge(target_train, how=\"inner\", on=\"id\")   \n",
    "    \n",
    "    X = df.copy().fillna(0)\n",
    "    y = df[\"target\"].copy()\n",
    "    \n",
    "    if(\"target\" in list(X.columns)):\n",
    "        del X['target']\n",
    "    if(\"id\" in list(X.columns)):\n",
    "        del X['id']\n",
    "    if(\"Unnamed: 0\" in list(X.columns)):\n",
    "        del X[\"Unnamed: 0\"]\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search con parametros tuenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_models(models, parameters, classiffiers, best_models_params, X_train, y_train):\n",
    "    \n",
    "    for key in classiffiers:\n",
    "        \n",
    "        clf = modelos[key]\n",
    "        params = parameters[key]\n",
    "        \n",
    "        best_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "        best_clf.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Model: \", key)\n",
    "        evaluate(best_clf, X_test, y_test)\n",
    "        print()\n",
    "        \n",
    "        best_models_params[key] = best_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametros de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_sin_tune = [LogisticRegression(C=1.0, max_iter=1000),SVC(),MultinomialNB(),DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(n_neighbors=5),RandomForestClassifier(),GaussianNB(),SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_XGB = {\n",
    "    \"criterion\": [\"entropy\",\"gini\"],\n",
    "    'n_estimators': np.arange(10,2000,200),\n",
    "    'max_features': ['auto', 'sqrt','log2'],\n",
    "    'max_depth': np.linspace(1, 500, 10, endpoint=True),\n",
    "    'min_samples_split': np.linspace(0.0, 1.0, 10, endpoint=True),\n",
    "    'min_samples_leaf': np.arange(0.0, 1,0.05),\n",
    "    'objective': ['binary:logistic'],\n",
    "    'learning_rate':np.arange(0.1,0.5,0.1),\n",
    "    'gamma':np.arange(0,0.5,0.1),\n",
    "    'subsample':np.arange(0.6,1,0.1),'colsample_bytree':np.arange(0.6,0.91,0.05),\n",
    "    'colsample_bylevel':np.arange(0.6,0.91,0.05),\n",
    "    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "    'tree_method':['auto', 'exact', 'approx', 'hist', 'gpu_hist']\n",
    "}\n",
    "\n",
    "\n",
    "params_LGBM ={\n",
    "    'objective': ['binary'],\n",
    "    'num_leaves': np.arange(25,70,4),\n",
    "    'learning_rate':[0.005,0.01,0.05,0.1,0.3],\n",
    "    'n_estimators': np.arange(25,200,15),\n",
    "    'max_depth': np.arange(5,13,1),\n",
    "    'min_split_gain': [0.001,0.01,0.1,0.2],\n",
    "    'bagging_fraction': np.arange(0.8,1.01,0.1),\n",
    "    'feature_fraction': np.arange(0.1,0.91,0.2)\n",
    "}\n",
    "\n",
    "\n",
    "params_RF = {\n",
    "    \"criterion\": [\"entropy\",\"gini\"],\n",
    "    'n_estimators': np.arange(10,2000,200),\n",
    "    'max_features': ['auto', 'sqrt','log2'],\n",
    "    'max_depth': np.linspace(1, 500, 10, endpoint=True),\n",
    "    'min_samples_split': np.linspace(0.0, 1.0, 10, endpoint=True),\n",
    "    'min_samples_leaf': np.arange(1, 10,1),\n",
    "}\n",
    "\n",
    "params_DT = {\n",
    "    \"criterion\": [\"entropy\",\"gini\"],\n",
    "    'max_features': ['auto', 'sqrt','log2'],\n",
    "    'max_depth': np.linspace(1, 500, 10, endpoint=True),\n",
    "    'min_samples_split': np.linspace(0.0, 1.0, 10, endpoint=True),\n",
    "    'min_samples_leaf': np.arange(0.0, 1,0.05),\n",
    "}\n",
    "\n",
    "params_ET = {\n",
    "    \"criterion\": [\"entropy\",\"gini\"],\n",
    "    'n_estimators': np.arange(10,2000,200),\n",
    "    'max_features': ['auto', 'sqrt','log2'],\n",
    "    'max_depth': np.linspace(1, 500, 10, endpoint=True),\n",
    "    'min_samples_split': np.linspace(0.0, 1.0, 10, endpoint=True),\n",
    "    'min_samples_leaf': np.arange(0.0, 1,0.05),\n",
    "}\n",
    "\n",
    "params_MLPC = {'hidden_layer_sizes':[(n,n,n) for n in range(1,30)],\n",
    "                              'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "                              'alpha':[1e-06,1e-05,1e-04,1e-03,1e-02,1e-01,1],\n",
    "                              'beta_1':np.arange(0.0,1,0.01),\n",
    "                              'beta_2':np.arange(0.0,1,0.01),\n",
    "                              'early_stopping':[True, False],\n",
    "                              'epsilon':[1e-07,1e-08,1e-06, 1e-09, 1e-010, 1e-11],\n",
    "                              'learning_rate':['constant', 'adaptive'],\n",
    "                              'solver':['adam', 'lbfgs', 'sgd'],\n",
    "                              'validation_fraction':np.arange(0.15,0.5,0.01),\n",
    "                               'max_iter':[200,300,400],\n",
    "                             }\n",
    "\n",
    "params_GBC = {\n",
    "    'max_features':['auto', 'sqrt', 'log2'],\n",
    "                             'max_leaf_nodes': [None,1,2,3,4,5,6,8],\n",
    "                             'min_weight_fraction_leaf': np.linspace(0.0, 0.5, 1, endpoint=True),\n",
    "                             'learning_rate': np.arange(0.1, 0.5, 0.05),\n",
    "                             'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "                             'min_samples_leaf': np.arange(0.1, 0.5, 5),  \n",
    "                             'max_features' : list(range(1,100)),\n",
    "                             'max_depth': [n for n in range(0,50,5)],\n",
    "                             'n_estimators': [n for n in range(0,1000,10)],\n",
    "                             'subsample': np.arange(0.3, 1,0.1),\n",
    "                             'loss': ['deviance'],\n",
    "                             'warm_start': [True, False],\n",
    "                             'presort': ['auto'],\n",
    "\n",
    "}\n",
    "\n",
    "params_LR = {\n",
    "    'penalty':['l1', 'l2', 'elasticnet'],\n",
    "    'C': np.arange(0.5,0.8,20),\n",
    "    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelos = {\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"LBGM\": LGBMClassifier(), \n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"ET\": ExtraTreesClassifier(), \n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"MLPC\": MLPClassifier(),\n",
    "    \"GBC\": GradientBoostingClassifier(),\n",
    "    \"LR\": LogisticRegression()\n",
    "}\n",
    "\n",
    "parametros = {\n",
    "    \"XGB\": params_XGB,\n",
    "    \"LBGM\": params_LGBM,\n",
    "    \"RF\": params_RF,\n",
    "    \"ET\": params_ET,\n",
    "    \"DT\": params_DT,\n",
    "    \"MLPC\": params_MLPC,\n",
    "    \"GBC\": params_GBC,\n",
    "    \"LR\": params_LR\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado para distintos features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features de bow limpieza profunda de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGO FEATURES\n",
    "\n",
    "df = pd.read_csv(\"features/feature_selection/features_cleaned_text_train.csv\")\n",
    "\n",
    "X,y = get_clean_values(df)\n",
    "\n",
    "corpus = list(X['text'].fillna(''))\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "\n",
    "#Split the data into train and test datasets for model training and testing\n",
    "X_train, X_test, y_train, y_test =train_test_split(bow,y, test_size=0.2,random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.3   0.7 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       849\n",
      "           1       0.86      0.69      0.77       674\n",
      "\n",
      "    accuracy                           0.82      1523\n",
      "   macro avg       0.83      0.80      0.81      1523\n",
      "weighted avg       0.82      0.82      0.81      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:97.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:82.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.3   0.7 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85       849\n",
      "           1       0.88      0.66      0.76       674\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.83      0.80      0.80      1523\n",
      "weighted avg       0.82      0.81      0.81      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:95.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:81.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.3   0.7 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       849\n",
      "           1       0.78      0.75      0.76       674\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.79      0.79      0.79      1523\n",
      "weighted avg       0.79      0.80      0.79      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:94.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:80.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.3   0.7 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80       849\n",
      "           1       0.77      0.67      0.72       674\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.77      0.76      0.76      1523\n",
      "weighted avg       0.77      0.76      0.76      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:100.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:76.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   1.0   0.0 \n",
      "    false   0.7   0.3 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77       849\n",
      "           1       0.88      0.33      0.48       674\n",
      "\n",
      "    accuracy                           0.68      1523\n",
      "   macro avg       0.76      0.64      0.62      1523\n",
      "weighted avg       0.75      0.68      0.64      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:75.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:68.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       849\n",
      "           1       0.85      0.61      0.71       674\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.80      0.76      0.77      1523\n",
      "weighted avg       0.79      0.78      0.77      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:100.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:78.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.5   0.5 \n",
      "    false   0.2   0.8 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.47      0.58       849\n",
      "           1       0.55      0.81      0.65       674\n",
      "\n",
      "    accuracy                           0.62      1523\n",
      "   macro avg       0.65      0.64      0.62      1523\n",
      "weighted avg       0.66      0.62      0.61      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:95.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:62.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.3   0.7 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       849\n",
      "           1       0.81      0.72      0.76       674\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:99.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:80.0%\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# PUEBO ALGORITMOS SIN TUNEAR\n",
    "\n",
    "fit_predict_models_sin_tune(X_test, X_train, y_test, y_train, models_sin_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUEBO CIERTOS ALGORITMOS TUNEADOS\n",
    "\n",
    "classiffiers = [ \"ET\", \"DT\", \"RF\", \"LBGM\",\"MLPC\", \"GBC\", 'LR']\n",
    "best_params_Text = {}\n",
    "grid_search_models(modelos, parametros, classiffiers, best_params_Text, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features seleccionados con Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGO FEATURES\n",
    "\n",
    "df = pd.read_csv(\"features/feature_selection/features_DecisionTreeClassiffier_BOW_SinTag_train.csv\")\n",
    "X,y = get_clean_values(df)\n",
    "\n",
    "#Split the data into train and test datasets for model training and testing\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.2,random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78       849\n",
      "           1       0.75      0.62      0.68       674\n",
      "\n",
      "    accuracy                           0.74      1523\n",
      "   macro avg       0.74      0.73      0.73      1523\n",
      "weighted avg       0.74      0.74      0.74      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:76.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:74.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.6   0.4 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.83      0.71       849\n",
      "           1       0.64      0.38      0.47       674\n",
      "\n",
      "    accuracy                           0.63      1523\n",
      "   macro avg       0.63      0.60      0.59      1523\n",
      "weighted avg       0.63      0.63      0.61      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:64.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:63.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.7   0.3 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.71       849\n",
      "           1       0.64      0.61      0.62       674\n",
      "\n",
      "    accuracy                           0.67      1523\n",
      "   macro avg       0.67      0.67      0.67      1523\n",
      "weighted avg       0.67      0.67      0.67      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:69.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:67.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.7   0.3 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71       849\n",
      "           1       0.63      0.62      0.63       674\n",
      "\n",
      "    accuracy                           0.67      1523\n",
      "   macro avg       0.67      0.67      0.67      1523\n",
      "weighted avg       0.67      0.67      0.67      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:99.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:67.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.7   0.3 \n",
      "    false   0.5   0.5 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70       849\n",
      "           1       0.61      0.50      0.55       674\n",
      "\n",
      "    accuracy                           0.64      1523\n",
      "   macro avg       0.63      0.62      0.62      1523\n",
      "weighted avg       0.63      0.64      0.63      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:77.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:64.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.86      0.79       849\n",
      "           1       0.77      0.60      0.67       674\n",
      "\n",
      "    accuracy                           0.74      1523\n",
      "   macro avg       0.75      0.73      0.73      1523\n",
      "weighted avg       0.75      0.74      0.74      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:99.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:74.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.6   0.4 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.92      0.75       849\n",
      "           1       0.77      0.35      0.48       674\n",
      "\n",
      "    accuracy                           0.67      1523\n",
      "   macro avg       0.70      0.63      0.62      1523\n",
      "weighted avg       0.70      0.67      0.63      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:69.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:67.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.5   0.5 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78       849\n",
      "           1       0.79      0.49      0.61       674\n",
      "\n",
      "    accuracy                           0.72      1523\n",
      "   macro avg       0.74      0.70      0.69      1523\n",
      "weighted avg       0.74      0.72      0.70      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:73.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:72.0%\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# PUEBO ALGORITMOS SIN TUNEAR\n",
    "\n",
    "fit_predict_models_sin_tune(X_test, X_train, y_test, y_train, models_sin_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   15.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ET\n",
      "Model Performance\n",
      "Accuracy = 56.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  DT\n",
      "Model Performance\n",
      "Accuracy = 62.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   17.5s remaining:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   18.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RF\n",
      "Model Performance\n",
      "Accuracy = 70.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LBGM\n",
      "Model Performance\n",
      "Accuracy = 76.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   51.0s finished\n",
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  MLPC\n",
      "Model Performance\n",
      "Accuracy = 74.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  GBC\n",
      "Model Performance\n",
      "Accuracy = 73.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   12.0s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   14.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LR\n",
      "Model Performance\n",
      "Accuracy = 74.00%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PRUEBO CIERTOS ALGORITMOS TUNEADOS\n",
    "\n",
    "classiffiers = [ \"ET\", \"DT\", \"RF\", \"LBGM\",\"MLPC\", \"GBC\", 'LR']\n",
    "best_params_DT = {}\n",
    "grid_search_models(modelos, parametros, classiffiers, best_params_DT, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features seleccionados con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGO FEATURES\n",
    "\n",
    "df = pd.read_csv(\"features/feature_selection/features_RandoForest_train.csv\")\n",
    "X,y = get_clean_values(df)\n",
    "\n",
    "#Split the data into train and test datasets for model training and testing\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y,test_size=0.2,random_state=2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anichu/Programs/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.78       849\n",
      "           1       0.75      0.58      0.65       674\n",
      "\n",
      "    accuracy                           0.73      1523\n",
      "   macro avg       0.73      0.71      0.71      1523\n",
      "weighted avg       0.73      0.73      0.72      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:77.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:73.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.6   0.4 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73       849\n",
      "           1       0.67      0.42      0.52       674\n",
      "\n",
      "    accuracy                           0.65      1523\n",
      "   macro avg       0.66      0.63      0.62      1523\n",
      "weighted avg       0.66      0.65      0.63      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:66.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:65.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.5   0.5 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.84      0.76       849\n",
      "           1       0.72      0.53      0.61       674\n",
      "\n",
      "    accuracy                           0.70      1523\n",
      "   macro avg       0.71      0.68      0.68      1523\n",
      "weighted avg       0.70      0.70      0.69      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:72.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:70.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.7   0.3 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.72       849\n",
      "           1       0.65      0.64      0.65       674\n",
      "\n",
      "    accuracy                           0.69      1523\n",
      "   macro avg       0.69      0.69      0.69      1523\n",
      "weighted avg       0.69      0.69      0.69      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:99.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:69.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.8   0.2 \n",
      "    false   0.5   0.5 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72       849\n",
      "           1       0.65      0.48      0.55       674\n",
      "\n",
      "    accuracy                           0.66      1523\n",
      "   macro avg       0.66      0.64      0.64      1523\n",
      "weighted avg       0.66      0.66      0.65      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:77.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:66.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.9   0.1 \n",
      "    false   0.4   0.6 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80       849\n",
      "           1       0.79      0.62      0.70       674\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.77      0.74      0.75      1523\n",
      "weighted avg       0.76      0.76      0.75      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:99.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:76.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   1.0   0.0 \n",
      "    false   0.6   0.4 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.97      0.78       849\n",
      "           1       0.89      0.36      0.51       674\n",
      "\n",
      "    accuracy                           0.70      1523\n",
      "   macro avg       0.77      0.66      0.65      1523\n",
      "weighted avg       0.76      0.70      0.66      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:71.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:70.0%\n",
      "**************************************************\n",
      "           true false \n",
      "     true   0.1   0.9 \n",
      "    false   0.0   1.0 \n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.12      0.21       849\n",
      "           1       0.47      0.97      0.63       674\n",
      "\n",
      "    accuracy                           0.50      1523\n",
      "   macro avg       0.65      0.54      0.42      1523\n",
      "weighted avg       0.67      0.50      0.40      1523\n",
      "\n",
      "--------------------------------------------------\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=500, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on training set:49.0%\n",
      "--------------------------------------------------\n",
      "Accuracy of classifier on test set:50.0%\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# PUEBO ALGORITMOS SIN TUNEAR\n",
    "\n",
    "fit_predict_models_sin_tune(X_test, X_train, y_test, y_train, models_sin_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   50.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ET\n",
      "Model Performance\n",
      "Accuracy = 65.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:    0.8s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  DT\n",
      "Model Performance\n",
      "Accuracy = 56.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   23.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RF\n",
      "Model Performance\n",
      "Accuracy = 70.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LBGM\n",
      "Model Performance\n",
      "Accuracy = 76.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   52.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  MLPC\n",
      "Model Performance\n",
      "Accuracy = 75.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  GBC\n",
      "Model Performance\n",
      "Accuracy = 70.00%.\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   11.6s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   13.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  LR\n",
      "Model Performance\n",
      "Accuracy = 73.00%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PRUEBO CIERTOS ALGORITMOS TUNEADOS\n",
    "\n",
    "classiffiers = [ \"ET\", \"DT\", \"RF\", \"LBGM\",\"MLPC\", \"GBC\", 'LR']\n",
    "best_params_RF = {}\n",
    "grid_search_models(modelos, parametros, classiffiers, best_params_RF, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features seleccionados con Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGO FEATURES\n",
    "\n",
    "df = pd.read_csv(\"features/feature_selection/features_ExtraTreesClassifier_BOW_SinTag_train.csv\")\n",
    "X,y = get_clean_values(df)\n",
    "\n",
    "#Split the data into train and test datasets for model training and testing\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.2,random_state=2020)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUEBO ALGORITMOS SIN TUNEAR\n",
    "\n",
    "fit_predict_models_sin_tune(X_test, X_train, y_test, y_train, models_sin_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUEBO CIERTOS ALGORITMOS TUNEADOS\n",
    "\n",
    "classiffiers = [ \"ET\", \"DT\", \"RF\", \"LBGM\",\"MLPC\", \"GBC\", 'LR']\n",
    "best_params_ET = {}\n",
    "grid_search_models(modelos, parameteros, classiffiers, best_params_ET, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
