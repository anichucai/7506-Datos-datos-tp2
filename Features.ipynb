{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, os, re, string, collections\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  imageio import imread\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import multidict as multidict #pip install multidict\n",
    "import itertools\n",
    "import csvtomd\n",
    "import re\n",
    "from collections import Counter\n",
    "#%matplotlib inline\n",
    "import spacy\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Limpieza y conversión de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_test = {\"id\": np.int32, \"keyword\": \"category\"}\n",
    "test = pd.read_csv(\"original_data/test.csv\", dtype = dtype_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_train = {\"id\": np.int32, \"keyword\": \"category\", \"target\" : bool}\n",
    "train = pd.read_csv(\"original_data/train.csv\", dtype = dtype_train, encoding='UTF_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-21a123804deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mw_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhitespaceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlemmatizer\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer  = nltk.stem.WordNetLemmatizer()\n",
    "    if text == '' or pd.isnull(text):\n",
    "        return text\n",
    "    else:\n",
    "        wordList = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "        s = ' '\n",
    "        return s.join(wordList)\n",
    "    \n",
    "def lemmatize_text_v2(text):\n",
    "    if text == '' or pd.isnull(text):\n",
    "        return text\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def clean_keyword_field(df):\n",
    "    df[\"keyword\"] = df[\"keyword\"].str.replace('%20',' ')\n",
    "    df_sin_keywords_nulos = df.dropna(subset = ['keyword'])\n",
    "    keywords = np.unique(df_sin_keywords_nulos.keyword)\n",
    "    for keyword in keywords:\n",
    "        df.loc[(df['text'].str.contains(keyword, case=False) & df['keyword'].isnull()), 'keyword'] = keyword\n",
    "    #df['keyword'] = df['keyword'].apply(lemmatize_text_v2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strict (for tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStopwordsList():\n",
    "    fileNamesList = [\"texts/99webtools.txt\", \"texts/atire_ncbi.txt\", \"texts/atire_puurula.txt\", \"texts/azure.txt\", \n",
    "                 \"texts/bbalet.txt\", \n",
    "                 \"texts/bow_short.txt\", \"texts/choi_2000naacl.txt\", \"texts/cook1988_function_words.txt\", \n",
    "                 \"texts/corenlp_acronym.txt\", \n",
    "                 \"texts/corenlp_hardcoded.txt\", \"texts/corenlp_stopwords.txt\", \"texts/datasciencedojo.txt\", \n",
    "                 \"texts/deeplearning4j.txt\", \n",
    "                 \"texts/dkpro.txt\", \"texts/mongodb.txt\", \"texts/galago_inquery.txt\", \"texts/gate_keyphrase.txt\", \n",
    "                 \"texts/gensim.txt\", \n",
    "                 \"texts/glasgow_stop_words.txt\", \"texts/indri.txt\", \"texts/kevinbouge.txt\", \"texts/lexisnexis.txt\",\n",
    "                 \"texts/lingpipe.txt\", \n",
    "                 \"texts/mallet.txt\", \"texts/mysql_innodb.txt\", \"texts/mysql_myisam.txt\", \"texts/galago_rmstop.txt\", \n",
    "                 \"texts/atire_ncbi.txt\", \n",
    "                 \"texts/galago_rmstop.txt\", \"texts/nltk.txt\", \"texts/okapiframework.txt\", \"texts/okapi_cacm_expanded.txt\", \n",
    "                 \"texts/onix.txt\", \n",
    "                 \"texts/ovid.txt\", \"texts/postgresql.txt\", \"texts/pubmed.txt\", \"texts/quanteda.txt\", \"texts/r_tm.txt\", \n",
    "                 \"texts/ranksnl_large.txt\", \n",
    "                 \"texts/reuters_wos.txt\", \"texts/rouge_155.txt\", \"texts/scikitlearn.txt\", \"texts/smart.txt\", \n",
    "                 \"texts/snowball_expanded.txt\", \n",
    "                 \"texts/spacy.txt\", \"texts/spark_mllib.txt\", \"texts/sphinx_mirasvit.txt\", \"texts/t101_minimal.txt\", \n",
    "                 \"texts/taporware.txt\", \n",
    "                 \"texts/terrier.txt\", \"texts/tonybsk_1.txt\", \"texts/tonybsk_6.txt\", \"texts/voyant_taporware.txt\", \n",
    "                 \"texts/weka.txt\", \n",
    "                 \"texts/xapian.txt\", \"texts/xpo6.txt\", \"texts/zettair.txt\"]\n",
    "    stopwordsList = []\n",
    "    for fileName in fileNamesList:\n",
    "        file = open(fileName, \"r\")\n",
    "        for line in file:\n",
    "            stripped_line = line. strip()\n",
    "            line_list = stripped_line\n",
    "            if line_list not in stopwordsList:\n",
    "                stopwordsList.append(line_list)\n",
    "        file.close()\n",
    "    return stopwordsList\n",
    "\n",
    "stopwordsList = getStopwordsList()\n",
    "\n",
    "print(len(stopwordsList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_strict(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\d+', '')\n",
    "    invalid_chars = ['#','|','@','!','?','-','_','[',']','%','&',':','.',',',\"''\",'/','https','(','//t',')','http',\n",
    "                 ';','\\'']\n",
    "    for char in invalid_chars:\n",
    "        if char in text:\n",
    "            text = text.replace(char,' ')\n",
    "    #removes url and tags\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \" \",  text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non strict (for special characters, hashtags, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_non_strict(text): \n",
    "    tw = \" \"\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        word = ''.join(filter(lambda x: x in set(string.printable), word))\n",
    "        word = word.replace(\"\\n\",\" \")\n",
    "        word = word.replace('û',\"\")\n",
    "        word = word.replace('Û',\"\")\n",
    "        #word = word.replace('_','')\n",
    "        #word = word.replace(\"\\\"\",'')\n",
    "        #word = word.strip('.')\n",
    "        #word = word.strip(',')\n",
    "        #word = word.strip(':')\n",
    "        tw += word + \" \"\n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIONES AUXILIARES: Limpieza de tweet\n",
    "\n",
    "# Función que saca los links\n",
    "\n",
    "def quitar_link_twitter(tweet):\n",
    "    res = []\n",
    "    for w in tweet.split(\" \"):\n",
    "        if ((\"http\" not in w) and (\"https\" not in w)):\n",
    "            res.append(w)\n",
    "    return (\" \").join(res)\n",
    "\n",
    "# Función que saca las menciones\n",
    "\n",
    "def quitar_mencion_twitter(tweet):\n",
    "    res = []\n",
    "    for w in tweet.split(\" \"):\n",
    "        if(\"@\" not in w):\n",
    "            res.append(w)\n",
    "    return (\" \").join(res)\n",
    "\n",
    "def quitar_simbolo(tweet, simbolo):\n",
    "    text = \"\"\n",
    "    for c in tweet:\n",
    "        if(c!=simbolo):\n",
    "            text+=c\n",
    "    return text\n",
    "\n",
    "def agregar_espacio_ente_comas(tweet):\n",
    "    \n",
    "    tweer_sin_coma = (\" \").join(tweet.split(\",\"))\n",
    "\n",
    "    return (\" \").join(tweer_sin_coma.split(\".\"))\n",
    "\n",
    "def capitalize_each_word(string):\n",
    "    \n",
    "    text = \"\"\n",
    "    for w in string.split(\" \"):\n",
    "        if(len(w)>3):\n",
    "            text += w.capitalize()+\" \"\n",
    "    return text\n",
    "\n",
    "def limpiar_tweet_location(tweet):\n",
    "    res = tweet\n",
    "    func = [quitar_mencion_twitter, quitar_link_twitter, agregar_espacio_ente_comas, lambda x: quitar_simbolo(x, \"#\"), capitalize_each_word]\n",
    "    for f in func:\n",
    "        res = f(res)\n",
    "    return res\n",
    "\n",
    "def to_string(a_list, delimeter):\n",
    "    res = \"\"\n",
    "    \n",
    "    for e in sorted(filter(None, a_list)):\n",
    "        res += str(e)+delimeter\n",
    "    \n",
    "    if(len(res)==0): return None\n",
    "    return res[: -len(delimeter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES AUXILIARES: Analisis geografico\n",
    "\n",
    "from geotext import GeoText\n",
    "import pycountry\n",
    "\n",
    "def get_country_pycountry(text):\n",
    "    for country in pycountry.countries:\n",
    "        # Handle both the cases(Uppercase/Lowercase)\n",
    "        if str(country.name).lower() in str(text).lower():\n",
    "            return GeoText(country.name).country_mentions\n",
    "\n",
    "def get_country_geotext(text):\n",
    "    return GeoText(text).country_mentions\n",
    "\n",
    "def get_country(text):\n",
    "    set1 = set(get_country_geotext(text))\n",
    "    countries = get_country_pycountry(text)\n",
    "    if(not countries):\n",
    "        return list(set1)\n",
    "    return list(set1.union(set(countries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo dataframe y limpio la location\n",
    "\n",
    "location_in_text = train[[\"id\", \"text\"]].copy()\n",
    "location_in_text['text'] = location_in_text['text'].apply(lambda x: limpiar_tweet_location(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo columnas con informacion\n",
    "\n",
    "location_in_text[\"cities\"] = location_in_text[\"text\"].apply(lambda x: GeoText(str(x)).cities)\n",
    "location_in_text[\"cities_str\"] = location_in_text[\"cities\"].apply(lambda x: to_string(x,\",\"))\n",
    "\n",
    "location_in_text[\"country\"] = location_in_text[\"text\"].apply(lambda x: get_country(x))\n",
    "location_in_text[\"country_str\"] = location_in_text[\"country\"].apply(lambda x: to_string(x,\",\"))\n",
    "\n",
    "location_in_text[\"has_country\"] = location_in_text[\"country\"].apply(lambda x: len(x)!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las columnas más útiles para crear features\n",
    "\n",
    "location_in_text = location_in_text[[\"id\",\"cities_str\",\"country_str\",\"has_country\"]]\n",
    "location_in_text.columns = [\"id\", \"has_location\", \"cities\", \"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_in_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location label info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo dataframe y limpio la location\n",
    "\n",
    "location = train[[\"id\", \"location\",\"target\"]].copy().fillna(\"\")\n",
    "location[\"location\"] = location[\"location\"].apply(lambda x: limpiar_tweet_location(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo columnas con informacion\n",
    "\n",
    "location[\"has_location\"] = location[\"location\"]!=\"\"\n",
    "\n",
    "location[\"cities\"] = location[\"location\"].apply(lambda x: GeoText(str(x)).cities)\n",
    "location[\"cities_str\"] = location[\"cities\"].apply(lambda x: to_string(x,\",\"))\n",
    "\n",
    "location[\"country\"] = location[\"location\"].apply(lambda x: get_country(x))\n",
    "location[\"country_str\"] = location[\"country\"].apply(lambda x: to_string(x,\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_aux = location[[\"country_str\", \"cities_str\"]].isnull()[520:525]\n",
    "location[\"city_country_not_found\"] = location_aux[\"country_str\"] & location_aux[\"cities_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las columnas más útiles para crear features\n",
    "\n",
    "location_info = location[[\"id\", \"has_location\", \"cities_str\", \"country_str\", \"city_country_not_found\"]]\n",
    "location_info.columns = [\"id\", \"has_location\", \"cities\", \"country\", \"city_country_not_found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv(\"new_data/links.csv\", encoding='UTF_8').set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "links['http']= links['http'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones auxiliares\n",
    "\n",
    "def get_dominio(links):\n",
    "    res= []\n",
    "    for l in links:\n",
    "        if(l==None):\n",
    "            res.append(None)\n",
    "        else:\n",
    "            dom = l.split(\"/\")\n",
    "            res.append(dom[2])\n",
    "    return res\n",
    "\n",
    "def type_http(links):\n",
    "    res = []\n",
    "    for l in links:\n",
    "        if(l==None):\n",
    "            res.append(None)\n",
    "        else:\n",
    "            dom = l.split(\":\")\n",
    "            res.append(dom[0])\n",
    "    return res\n",
    "\n",
    "def count_None(a_list):\n",
    "    count = 0\n",
    "    for l in a_list:\n",
    "        if(l==None):\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def link_encription_generalize(link_encript):\n",
    "    \n",
    "    has_http = False\n",
    "    has_https = False\n",
    "    \n",
    "    for type_http in link_encript:\n",
    "        if type_http==None: \n",
    "            continue\n",
    "        elif type_http== \"https\":\n",
    "            has_https = True\n",
    "        else:\n",
    "            has_http = True\n",
    "    \n",
    "    if(has_http and has_https):\n",
    "        return \"both\"\n",
    "    elif(has_http):\n",
    "        return \"http\"\n",
    "    elif(has_https):\n",
    "        return \"https\"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def to_string(a_list, delimeter):\n",
    "    res = \"\"\n",
    "    \n",
    "    for e in sorted(filter(None, a_list)):\n",
    "        res += str(e)+delimeter\n",
    "    \n",
    "    if(len(res)==0): return None\n",
    "    return res[: -len(delimeter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links[\"links_domain\"] = links[\"http\"].apply(lambda x: get_dominio(x))\n",
    "links[\"links_encription\"] = links[\"http\"].apply(lambda x: type_http(x))\n",
    "links[\"links_encription_type\"] = links[\"links_encription\"].apply(lambda x: link_encription_generalize(x))\n",
    "links[\"cant_failed_links\"] = links[\"http\"].apply(lambda x: count_None(x))\n",
    "links[\"links_domain_str\"] = links[\"links_domain\"].apply(lambda x: to_string(x,\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de los campos para obtener tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES AUXILIARES\n",
    "\n",
    "def prettify(word_list):\n",
    "    wl = []\n",
    "    for word in word_list:\n",
    "        word = re.sub(r'[^A-Za-z]', \" \",  word)\n",
    "        word = re.sub(r'\\b\\w\\b',' ', word)\n",
    "        word = re.sub(r'\\s+', ' ', word)\n",
    "        wl.append(word.strip())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el dataframe\n",
    "df_tags = train[['id','text','target']].copy(deep=True)\n",
    "\n",
    "# Limpio el dataframe\n",
    "df_tags['new'] = df_tags[\"text\"].apply(lambda x: clean_text_strict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Eliminacion de terminos, caracteres y stopwords innecesarios para generar los tags\n",
    "#convierte de list a string\n",
    "\n",
    "df_tags['new'] = df_tags['new'].apply(lambda x: [item for item in x.split() if item not in stopwordsList])\n",
    "df_tags.new = df_tags.new.apply(prettify)\n",
    "\n",
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCION AUXILIAR\n",
    "def lemmatize_text2(word_list):\n",
    "    text = ' '\n",
    "    text = text.join(word_list)\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "df_tags['new_tokenized'] = df_tags.new.apply(lemmatize_text2)\n",
    "df_tags.new_tokenized = df_tags.new_tokenized.apply(prettify)\n",
    "\n",
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de ocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(word_list):\n",
    "    text = ' '\n",
    "    text = text.join(word_list)\n",
    "    return text\n",
    "\n",
    "#metodo para analizar ocurrencias y cantidades\n",
    "a = df_tags['new_tokenized'].apply(getText).str.lower().str.cat(sep=' ')\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(a)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "\n",
    "this_col = pd.Series(['word','frequency'])\n",
    "\n",
    "rslt = pd.DataFrame(word_dist.most_common(400),columns = this_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = rslt ['word']\n",
    "unique_frequent_words = set(frequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se genera columna con tags a partir de una cantidad de ocurrencias destacadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_finder(x):\n",
    "    x = set(x)\n",
    "    return list(unique_frequent_words.intersection(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags['tags'] = df_tags.new_tokenized.apply(word_finder)\n",
    "tags_info = df_tags[['id','target','text','tags']]\n",
    "\n",
    "tags_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords = clean_keyword_field(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_lemmatized = df_keywords.copy()\n",
    "df_keywords_lemmatized.keyword = df_keywords.keyword.apply(lambda x: lemmatize_text_v2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toText(listOfWords):\n",
    "    s=' '\n",
    "    return s.join(listOfWords)\n",
    "\n",
    "def textContainsKeyword(row):\n",
    "    if row.keyword == '' or pd.isnull(row.keyword):\n",
    "        return False\n",
    "    else:\n",
    "        return row.keyword.lower() in row.text.lower()\n",
    "    \n",
    "def getKeywords(df):\n",
    "    df_sin_keywords_nulos = df.dropna(subset = ['keyword'])\n",
    "    return np.unique(df_sin_keywords_nulos.keyword)\n",
    "\n",
    "def textContainsKeywordAsHashtag(row):\n",
    "    hashtags = []\n",
    "    tweet = row.text.lower()\n",
    "    if row.keyword == '' or pd.isnull(row.keyword):\n",
    "        return False\n",
    "    else:\n",
    "        for word in tweet.split(' '):\n",
    "            if (len(word)>1) and (word[0] == '#'):\n",
    "                hashtags.append(word[1:])\n",
    "        return row.keyword.lower() in hashtags\n",
    "\n",
    "def linkContainsKeyword(row):\n",
    "    links   = row.http\n",
    "    keyword = row.keyword\n",
    "    if row.keyword == '' or pd.isnull(row.keyword):\n",
    "        return False\n",
    "    if (type(links)==list):\n",
    "        for link in links:\n",
    "            if link=='' or pd.isnull(link):\n",
    "                continue\n",
    "            if keyword in link:\n",
    "                return True\n",
    "    else:\n",
    "        return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = pd.get_dummies(df_keywords_lemmatized[['id','keyword']],'keyword')\n",
    "all_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_keywords = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sin_keywords_nulos = df_keywords.dropna(subset=['keyword'])\n",
    "#%time df_keywords_not_in_text = df_sin_keywords_nulos[~df_sin_keywords_nulos['text'].str.contains(\"|\".join(df_sin_keywords_nulos['keyword']),case=False)]\n",
    "#%time df_sin_keywords_nulos[df_sin_keywords_nulos.apply(lambda x: x.keyword in x.text, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords['text_contains_keyword'] = df_keywords.apply(textContainsKeyword,axis=1)\n",
    "df_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords['has_keyword'] = ~df_keywords.keyword.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = getKeywords(df_keywords)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_lemmatized = getKeywords(df_keywords_lemmatized)\n",
    "keywords_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countKeywords(tweet):\n",
    "    quantity = 0\n",
    "    tweet = re.sub('[^a-zA-Z\\ ]+', \"\", tweet).lower()\n",
    "    for word in tweet.split(' '):\n",
    "        #word = re.sub('[^a-zA-Z]+', '', word)\n",
    "        if word in keywords:\n",
    "            quantity+=1\n",
    "    return quantity\n",
    "\n",
    "def getHiddenKeywords(tweet):\n",
    "    k = []\n",
    "    tweet = re.sub('[^a-zA-Z\\ ]+', \"\", tweet).lower()\n",
    "    for keyword in keywords:\n",
    "        if keyword in tweet:\n",
    "            k.append(lemmatize_text_v2(keyword))\n",
    "    return k\n",
    "\n",
    "df_keywords['keywords_quantity'] = df_keywords.text.apply(countKeywords)\n",
    "df_keywords['keywords_in_text'] = df_keywords.text.apply(getHiddenKeywords)\n",
    "df_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords['keywords_mean'] = df_keywords.text.apply(countKeywords)/keywords.size\n",
    "df_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords['keyword_is_hashtag'] = df_keywords.apply(textContainsKeywordAsHashtag,axis=1)\n",
    "df_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# busco frecuencia de cada keyword para asignar un 'peso'\n",
    "df_keywords['keyword_frequency'] = df_keywords_lemmatized.groupby('keyword')['id'].transform('count') / keywords_lemmatized.size\n",
    "df_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_links = links.reset_index()[['http','id']]\n",
    "df_keywords_links = df_keywords.merge(just_links, on='id', how='left')\n",
    "df_keywords_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords['link_contains_keyword'] = df_keywords_links.apply(linkContainsKeyword,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords['keyword_lemmatized'] = df_keywords_lemmatized['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "df_keywords = df_keywords.join(pd.DataFrame(mlb.fit_transform(df_keywords.pop('keywords_in_text')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df_keywords.index).add_prefix('K_'))\n",
    "df_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords.drop(columns=['location', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrmap = df_keywords.corr()\n",
    "#fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "#sns.heatmap(corrmap, vmax=.8, square=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones auxiliares\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "def stop(text):\n",
    "    return [w for w in text.split() if w in stopword]\n",
    "\n",
    "def length(text):\n",
    "    return(np.mean([len(w) for w in text.split()]))\n",
    "\n",
    "def punct(text):\n",
    "    return [c for c in text if c in string.punctuation]\n",
    "\n",
    "def title(text):\n",
    "    return [w for w in text.split() if w.istitle()]\n",
    "\n",
    "def upper_list(text):\n",
    "    return [w for w in text.split() if w.isupper()]\n",
    "\n",
    "def lower_list(text):\n",
    "    return [w for w in text.split() if w.islower()]\n",
    "\n",
    "def count_special_char(text):\n",
    "    special_char = 0\n",
    "    text = text.replace(\" \", \"\")\n",
    "    for i in range(0, len(text)):\n",
    "        ch = text[i]\n",
    "        if (ch.isalpha()):  \n",
    "            continue\n",
    "        elif (ch.isdigit()):\n",
    "            continue\n",
    "        else:\n",
    "            special_char += 1\n",
    "    return special_char\n",
    "\n",
    "def syllables(text):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    for word in text:\n",
    "        word= word.lower()\n",
    "        if word[0] in vowels:\n",
    "            count +=1\n",
    "        for index in range(1,len(word)):\n",
    "            if word[index] in vowels and word[index-1] not in vowels:\n",
    "                count +=1\n",
    "        if word.endswith('e'):\n",
    "            count -= 1\n",
    "        if word.endswith('le'):\n",
    "            count+=1\n",
    "        if count == 0:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def quitar_link_twitter(tweet):\n",
    "    res = []\n",
    "    for w in tweet.split(\" \"):\n",
    "        if ((\"http\" not in w) and (\"https\" not in w)):\n",
    "            res.append(w)\n",
    "    return (\" \").join(res)\n",
    "\n",
    "def quitar_mencion_twitter(tweet):\n",
    "    res = []\n",
    "    for w in tweet.split(\" \"):\n",
    "        if(\"@\" not in w):\n",
    "            res.append(w)\n",
    "    return (\" \").join(res)\n",
    "\n",
    "def getHashtags(tweet):\n",
    "    pat = re.compile(r\"#(\\w+)\")\n",
    "    return pat.findall(tweet)\n",
    "\n",
    "def getTaggedUsers(tweet):\n",
    "    pat = re.compile(r\"@(\\w+)\")\n",
    "    return pat.findall(tweet)\n",
    "\n",
    "def agregar_features_cantidad_palabras(df_og):\n",
    "    df_og['text'] = df_og['text'].apply(lambda x: quitar_link_twitter(x))\n",
    "    df_og['text'] = df_og['text'].apply(lambda x: quitar_mencion_twitter(x))\n",
    "    palabras_list = [df_og]\n",
    "    for df in palabras_list:\n",
    "        # Número de palabras usadas\n",
    "        df['#palabras'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        # Número de palabras únicas\n",
    "        df['#palabras_unicas'] =  df['text'].apply(lambda x: len(set(x.split())))\n",
    "        # Número de caracteres\n",
    "        df['#caracteres'] =  df['text'].apply(lambda x: len(x))\n",
    "        # Número de stopwords\n",
    "        df['#stopwords'] = df['text'].apply(lambda x: len(stop(x)))\n",
    "        # Número de caracteres de puntuación\n",
    "        df['#puntuacion'] = df['text'].apply(lambda x: len(punct(x)))\n",
    "        # Número de palabras Capitalizadas\n",
    "        df['#capitalize'] = df['text'].apply(lambda x: len(title(x)))\n",
    "        # Número de palabras MAYUSCULAS\n",
    "        df['#mayusculas'] = df['text'].apply(lambda x: len(upper_list(x)))\n",
    "        # Número de silabas\n",
    "        df['#silabas'] = df['text'].apply(lambda x: syllables(x))\n",
    "        # Promedio de longitud del tweet\n",
    "        df['promedio_len_word'] = df['text'].apply(lambda x: length(x))\n",
    "        # Número de caracteres especiales\n",
    "        df['#caracteres_especiales'] = df['text'].apply(lambda x: count_special_char(x))\n",
    "    return df\n",
    "\n",
    "def getUniqueHashtagsList(hashtags):\n",
    "    hashtagList = []\n",
    "    for hashtag in hashtags:\n",
    "        for h in hashtag:\n",
    "            if h not in hashtagList:\n",
    "                hashtagList.append(h.lower())\n",
    "    return hashtagList\n",
    "\n",
    "def getHashtagsList(hashtags):\n",
    "    hashtagList = []\n",
    "    for hashtag in hashtags:\n",
    "        for h in hashtag:\n",
    "            hashtagList.append(h.lower())\n",
    "    return hashtagList\n",
    "\n",
    "def getMostFrequentHashtag(hashtags):\n",
    "    maxValue = 0\n",
    "    finalHashtag = ''\n",
    "    if (len(hashtags) > 0):\n",
    "        for hashtag in hashtags:\n",
    "            hashtag = hashtag.lower()\n",
    "            if (hashtagDict.get(hashtag) > maxValue):\n",
    "                finalHashtag = hashtag\n",
    "                maxValue = hashtagDict.get(hashtag)\n",
    "\n",
    "    return finalHashtag\n",
    "\n",
    "def getHashtagFrequency(hashtag):\n",
    "    if hashtag != '':\n",
    "        return hashtagDict.get(hashtag)/len(hashtagDict)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def getUniqueUsersList(users):\n",
    "    userList = []\n",
    "    for user in users:\n",
    "        for u in user:\n",
    "            if u not in userList:\n",
    "                userList.append(u.lower())\n",
    "    return userList\n",
    "\n",
    "def getUsersList(users):\n",
    "    userList = []\n",
    "    for user in users:\n",
    "        for u in user:\n",
    "            userList.append(u.lower())\n",
    "    return userList\n",
    "\n",
    "def hasRelevantTaggedUser(users):\n",
    "    minValue = 5\n",
    "    #finalUser = ''\n",
    "    if (len(users) > 0):\n",
    "        for user in users:\n",
    "            user = user.lower()\n",
    "            if (userDict.get(user) > minValue):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.text = df_text.text.apply(clean_text_non_strict)\n",
    "df_text['tagged_users'] = df_text.text.apply(lambda x: getTaggedUsers(x))\n",
    "df_text['#tagged_users'] = df_text.tagged_users.apply(lambda x: len(x))\n",
    "df_text['has_tagged_user'] = df_text['#tagged_users']>0\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = getUniqueUsersList(df_text.tagged_users)\n",
    "userDict = Counter(getUsersList(df_text.tagged_users))\n",
    "df_text['has_relevant_tagged_user'] = df_text.tagged_users.apply(lambda x:hasRelevantTaggedUser(x))\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = agregar_features_cantidad_palabras(df_text)\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning numerical data (esta super arbitrario esto)\n",
    "df_text['#palabras_binned'] = pd.qcut(df_text['#palabras'], q=4, labels=False)\n",
    "df_text['#palabras_unicas_binned'] = pd.qcut(df_text['#palabras_unicas'], q=4, labels=False)\n",
    "df_text['#caracteres_binned'] = pd.qcut(df_text['#caracteres'], q=5, labels=False)\n",
    "df_text['#stopwords_binned']  = pd.qcut(df_text['#stopwords'], q=3, labels=False)\n",
    "df_text['#puntuacion_binned'] = pd.qcut(df_text['#puntuacion'], q=3, labels=False)\n",
    "df_text['#capitalize_binned'] = pd.qcut(df_text['#capitalize'], q=3, labels=False)\n",
    "df_text['#mayusculas_binned'] = pd.qcut(df_text['#mayusculas'], q=2, labels=False, duplicates='drop')\n",
    "df_text['#silabas_binned']    = pd.qcut(df_text['#silabas'], q=4, labels=False)\n",
    "df_text['#caracteres_especiales_binned']    = pd.qcut(df_text['#caracteres_especiales'], q=3, labels=False)\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text['hashtags'] = df_text.text.apply(lambda x: getHashtags(x))\n",
    "df_text['#hashtags'] = df_text.hashtags.apply(lambda x: len(x))\n",
    "df_text['has_hashtag'] = df_text['#hashtags']>0\n",
    "hashtags = getUniqueHashtagsList(df_text.hashtags)\n",
    "hashtagDict = Counter(getHashtagsList(df_text.hashtags))\n",
    "\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text['relevant_hashtag'] = df_text.hashtags.apply(lambda x: getMostFrequentHashtag(x))\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text['freq_relevant_hashtag'] = df_text.relevant_hashtag.apply(lambda x:getHashtagFrequency(x))\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same location in label and in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge info location in text and in columnn \"location\"\n",
    "location_merged = location_in_text.merge(location_info, how=\"left\", on=\"id\",suffixes = ('_in_label', '_in_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location label countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para aquellos tweets con más de dos paises, separo en columnas\n",
    "\n",
    "countries = location['country_str'].str.split(\",\", expand = True)\n",
    "\n",
    "countries[\"id\"] = location[\"id\"]\n",
    "countries.set_index(\"id\")\n",
    "\n",
    "countries1 = pd.get_dummies(countries[0],\"Country\")\n",
    "countries2 = pd.get_dummies(countries[1],\"Country\")\n",
    "\n",
    "# Uno las columnas\n",
    "for country in countries1.columns:\n",
    "    if(country in countries2.columns):\n",
    "        countries1[country] = countries1[country] + countries2[country]\n",
    "for country in countries2.columns:\n",
    "    if(country in countries1.columns):\n",
    "        countries1[country] = countries2[country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW: Coutry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_country_index = location['country_str'].dropna().index\n",
    "corpus = list(location['country_str'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_vectorizer = CountVectorizer()\n",
    "\n",
    "bow_country = country_vectorizer.fit_transform(corpus).todense()\n",
    "df_bow_country = pd.DataFrame(bow_country)\n",
    "df_bow_country.set_index(bow_country_index)\n",
    "df_bow_country.columns = country_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf: Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Smoothing:\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
    "\n",
    "#create dataframe\n",
    "df_tf_idf_country=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "df_tf_idf_country.set_index(bow_country_index)\n",
    "\n",
    "df_tf_idf_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Smoothing:\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
    "\n",
    "#create dataframe\n",
    "df_tf_idf_smooth_country=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "df_tf_idf_smooth_country.set_index(bow_country_index)\n",
    "\n",
    "df_tf_idf_smooth_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features con cantidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_info = links[[\"links_cant\", \"cant_failed_links\", \"links_encription_type\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features con cantidad de apariciones de n dominios más usuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n cantidad de links\n",
    "n_most_common_links = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dejo en cada columna un links\n",
    "\n",
    "links_column = links['links_domain_str'].str.split(\",\", expand = True)\n",
    "links_column[105:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_cant_domain = pd.get_dummies(links_column[0],\"domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumo las columnas de acuerdo al link\n",
    "\n",
    "unique_domains = set(links_cant_domain.columns)\n",
    "\n",
    "for i in range(1, len(links_column.columns)):\n",
    "    n_links_pd = pd.get_dummies(links_column[i],\"domain\")\n",
    "    for domain in unique_domains:\n",
    "        if(domain in set(n_links_pd.columns)):\n",
    "            links_cant_domain[domain] = links_cant_domain[domain] + n_links_pd[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links_cant_domain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n most common domains\n",
    "n_domains = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculo los dominios más usuales\n",
    "unique_domains_ordered = links_column[0].value_counts().index\n",
    "columns_n_unique_links = [\"domain_\"+link for link in unique_domains_ordered[:n_domains]]\n",
    "\n",
    "most_common_domain_cant = links_cant_domain[columns_n_unique_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_common_domain_cant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW: Dominios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_domain_index = links['links_domain_str'].dropna().index\n",
    "corpus = list(links['links_domain_str'].dropna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "bow_domain = domain_vectorizer.fit_transform(corpus).todense()\n",
    "df_bow_domain = pd.DataFrame(bow_domain)\n",
    "df_bow_domain.set_index(bow_domain_index)\n",
    "df_bow_domain.columns = domain_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf:  Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_domain_index = links['links_domain_str'].dropna().index\n",
    "corpus = list(links['links_domain_str'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Smoothing:\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
    "\n",
    "#create dataframe\n",
    "df_tf_idf_domain=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "df_tf_idf_domain.set_index(bow_domain_index)\n",
    "\n",
    "df_tf_idf_domain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Smoothing:\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
    "\n",
    "#create dataframe\n",
    "df_tf_idf_smooth_domain=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "df_tf_idf_smooth_domain.set_index(bow_domain_index)\n",
    "\n",
    "df_tf_idf_smooth_domain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informacion general tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_info = tags_info.copy()\n",
    "tags_info[\"cant_tags\"] = tags_info[\"tags\"].apply(lambda x: len(x))\n",
    "tags_info[\"tags_str\"] = tags_info[\"tags\"].apply(lambda x: to_string(x,\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Nuevo feature por tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_columns = tags_info['tags_str'].str.split(\",\", expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_has_tags = pd.get_dummies(tags_columns[0],\"tag\")\n",
    "unique_tag = set(df_has_tags.columns)\n",
    "\n",
    "for i in range(1, len(tags_columns.columns)):\n",
    "    n_links_pd = pd.get_dummies(tags_columns[i],\"tag\")\n",
    "    for domain in unique_tag:\n",
    "        if(domain in set(tags_columns.columns)):\n",
    "            df_has_tags[domain] = df_has_tags[domain] + tags_columns[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_has_tags[\"id\"] = tags_info[\"id\"]\n",
    "df_has_tags = df_has_tags.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_has_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW: tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tags_index = tags_info['tags_str'].dropna().index\n",
    "corpus = list(tags_info['tags_str'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "bow_tags = domain_vectorizer.fit_transform(corpus).todense()\n",
    "df_bow_tags = pd.DataFrame(bow_tags)\n",
    "df_bow_tags.index = bow_tags_index\n",
    "df_bow_tags.columns = domain_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf: tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tags_index = tags_info['tags_str'].dropna().index\n",
    "corpus = list(tags_info['tags_str'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Smoothing:\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
    "\n",
    "#create dataframe\n",
    "df_tf_idf_tags=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "df_tf_idf_tags.set_index(bow_tags_index)\n",
    "\n",
    "df_tf_idf_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Smoothing:\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(corpus)\n",
    "\n",
    "#create dataframe\n",
    "df_tf_idf_smooth_tags =pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "df_tf_idf_smooth_tags.set_index(bow_tags_index)\n",
    "\n",
    "df_tf_idf_smooth_tags.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
